{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d429aa9f-9678-4eeb-a018-c596d922ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from quickda.explore_data import *\n",
    "from quickda.clean_data import *\n",
    "from quickda.explore_numeric import *\n",
    "from quickda.explore_categoric import *\n",
    "from quickda.explore_numeric_categoric import *\n",
    "from quickda.explore_time_series import *\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f639665a-95e4-498b-b938-76585fabd691",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9ad9c1-42f1-460d-b828-40e923180d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressaoLinear(X, Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    lm = LinearRegression()\n",
    "    linear = lm.fit(X_train, y_train.ravel())\n",
    "    print('Score Modelo: {}'.format(linear.score(X_test, y_test)))\n",
    "    y_pred = lm.predict(X_test)\n",
    "    print('RMSE (root-mean-squared error): {}'.format(math.sqrt(mean_squared_error(y_test,y_pred))))\n",
    "    \n",
    "def MatrizCorrelacao(df):\n",
    "    corr_matrix = df.corr()\n",
    "    plt.figure(figsize = (15,10))\n",
    "    sns.heatmap(corr_matrix, cmap=\"Reds\", center=0, annot=True)\n",
    "    return corr_matrix\n",
    "\n",
    "def RemoverOutliers(df):\n",
    "    for column in df.columns:\n",
    "        data = df[column]\n",
    "\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        minimum = Q1 - (1.5 * IQR)\n",
    "        maximum = Q3 + (1.5 * IQR)\n",
    "\n",
    "        outliers = ((data < minimum) | (data > maximum))\n",
    "        df[column].loc[outliers] = np.nan\n",
    "        \n",
    "    return df.dropna()\n",
    "\n",
    "def plot_history(history):\n",
    "    mse = history.history['mse']\n",
    "    val_mse = history.history['val_mse']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(mse) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.ylim(0,6000000000)\n",
    "    plt.plot(x, mse, 'b', label='Training MSE')\n",
    "    plt.plot(x, val_mse, 'r', label='Validation MSE')\n",
    "    plt.title('Training and validation MSE')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.ylim(0,100000)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f56f5bd-dc4c-48eb-8024-55045c6a64ac",
   "metadata": {},
   "source": [
    "## Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247b6ee9-6ecc-4582-af03-6d8f1cfd7289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8973 entries, 0 to 8972\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   maiorAtraso                  8973 non-null   int64  \n",
      " 1   margemBrutaAcumulada         8973 non-null   float64\n",
      " 2   prazoMedioRecebimentoVendas  8973 non-null   int64  \n",
      " 3   titulosEmAberto              8973 non-null   float64\n",
      " 4   valorSolicitado              8973 non-null   float64\n",
      " 5   percentualRisco              8973 non-null   float64\n",
      " 6   valorAprovado                7569 non-null   float64\n",
      " 7   ativoCirculante              8973 non-null   float64\n",
      " 8   passivoCirculante            8973 non-null   float64\n",
      " 9   totalAtivo                   8973 non-null   float64\n",
      " 10  totalPatrimonioLiquido       8973 non-null   float64\n",
      " 11  endividamento                8973 non-null   float64\n",
      " 12  duplicatasAReceber           8973 non-null   float64\n",
      " 13  estoque                      8973 non-null   float64\n",
      " 14  faturamentoBruto             8973 non-null   float64\n",
      " 15  margemBruta                  8973 non-null   float64\n",
      " 16  custos                       8973 non-null   float64\n",
      " 17  capitalSocial                8973 non-null   float64\n",
      " 18  scorePontualidade            8973 non-null   float64\n",
      " 19  limiteEmpresaAnaliseCredito  8973 non-null   float64\n",
      "dtypes: float64(18), int64(2)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"base_exe3.csv\");\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69735ec5-7a97-4b7c-abf7-831c5df6e681",
   "metadata": {},
   "source": [
    "Separamos solicitações aprovadas das em Análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0028b0-f098-4376-a186-4a19248d789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analise = df[df.valorAprovado.isnull()]\n",
    "df_aprovadas = df[~df.valorAprovado.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a378f-5876-4924-9d86-b47f9cf90350",
   "metadata": {},
   "source": [
    "Separando as informações da variável meta e variáveis preditoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8a1cab-1868-4632-aa18-05f6736eabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_aprovadas.valorAprovado\n",
    "X = df_aprovadas.drop(['valorAprovado'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d8190eb-9573-497d-98f8-65dd6a3c7161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Modelo: 0.17495846112389313\n",
      "RMSE (root-mean-squared error): 456207.50537551526\n"
     ]
    }
   ],
   "source": [
    "RegressaoLinear(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35be930-23ef-43ff-8e20-d1c407bc6339",
   "metadata": {},
   "source": [
    "Obtemos um RMSE de 456207 e um score baixo de 0.17<br>\n",
    "Como a Regressão tem um impacto grande com OutLiers iremos eles agora e treinar novamente o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03290b38-5aaf-4047-9627-1d660968be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sem_Outliers = RemoverOutliers(df_aprovadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0908f2f8-e9c9-4e9c-8aad-3f77bb530656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3352 entries, 0 to 8960\n",
      "Data columns (total 20 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   maiorAtraso                  3352 non-null   float64\n",
      " 1   margemBrutaAcumulada         3352 non-null   float64\n",
      " 2   prazoMedioRecebimentoVendas  3352 non-null   float64\n",
      " 3   titulosEmAberto              3352 non-null   float64\n",
      " 4   valorSolicitado              3352 non-null   float64\n",
      " 5   percentualRisco              3352 non-null   float64\n",
      " 6   valorAprovado                3352 non-null   float64\n",
      " 7   ativoCirculante              3352 non-null   float64\n",
      " 8   passivoCirculante            3352 non-null   float64\n",
      " 9   totalAtivo                   3352 non-null   float64\n",
      " 10  totalPatrimonioLiquido       3352 non-null   float64\n",
      " 11  endividamento                3352 non-null   float64\n",
      " 12  duplicatasAReceber           3352 non-null   float64\n",
      " 13  estoque                      3352 non-null   float64\n",
      " 14  faturamentoBruto             3352 non-null   float64\n",
      " 15  margemBruta                  3352 non-null   float64\n",
      " 16  custos                       3352 non-null   float64\n",
      " 17  capitalSocial                3352 non-null   float64\n",
      " 18  scorePontualidade            3352 non-null   float64\n",
      " 19  limiteEmpresaAnaliseCredito  3352 non-null   float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 549.9 KB\n"
     ]
    }
   ],
   "source": [
    "df_sem_Outliers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb4aee-b6fc-495a-82ac-3ed45405452b",
   "metadata": {},
   "source": [
    "Separando as informações da variável meta e variáveis preditoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12fec16b-059d-4ba0-b5ed-a223ceb8ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sem_Outliers.drop(['valorAprovado'], axis=1)\n",
    "Y = df_sem_Outliers.valorAprovado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4c16072-8591-4bb2-943d-bad085be1754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Modelo: 0.736276658716232\n",
      "RMSE (root-mean-squared error): 14297.672250170379\n"
     ]
    }
   ],
   "source": [
    "RegressaoLinear(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f76938b-4820-446b-8b17-56243f1bc768",
   "metadata": {},
   "source": [
    "Obtemos um RMSE de 14297 e um score baixo de 0.73<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a4140edc-99d5-42e1-b9a1-4e89a3c80c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StandardScaler()\n",
    "x_ssc = ssc.fit_transform(X)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_ssc, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c56efaad-694d-4c8d-9bf6-8e7d218f4da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               2560      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 76,801\n",
      "Trainable params: 76,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=19, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccd7d1d8-a25b-4633-8ff2-6ac1836f2050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "51/71 [====================>.........] - ETA: 0s - loss: 32075.7852 - mse: 1912777088.0000 - mae: 32075.7852\n",
      "Epoch 00001: val_mae improved from inf to 23242.49023, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 6ms/step - loss: 31156.2266 - mse: 1839353344.0000 - mae: 31156.2266 - val_loss: 23242.4902 - val_mse: 1227766784.0000 - val_mae: 23242.4902\n",
      "Epoch 2/100\n",
      "49/71 [===================>..........] - ETA: 0s - loss: 17659.0859 - mse: 750195456.0000 - mae: 17659.0859\n",
      "Epoch 00002: val_mae improved from 23242.49023 to 12759.30664, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 16761.8203 - mse: 672616832.0000 - mae: 16761.8203 - val_loss: 12759.3066 - val_mse: 362725792.0000 - val_mae: 12759.3066\n",
      "Epoch 3/100\n",
      "56/71 [======================>.......] - ETA: 0s - loss: 13483.0703 - mse: 439995360.0000 - mae: 13483.0703\n",
      "Epoch 00003: val_mae improved from 12759.30664 to 11584.46875, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 13345.5762 - mse: 422289376.0000 - mae: 13345.5762 - val_loss: 11584.4688 - val_mse: 298469952.0000 - val_mae: 11584.4688\n",
      "Epoch 4/100\n",
      "53/71 [=====================>........] - ETA: 0s - loss: 12286.6426 - mse: 343115520.0000 - mae: 12286.6426\n",
      "Epoch 00004: val_mae improved from 11584.46875 to 11016.21094, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 12094.7373 - mse: 334588320.0000 - mae: 12094.7373 - val_loss: 11016.2109 - val_mse: 265339072.0000 - val_mae: 11016.2109\n",
      "Epoch 5/100\n",
      "56/71 [======================>.......] - ETA: 0s - loss: 12137.7021 - mse: 338319520.0000 - mae: 12137.7021\n",
      "Epoch 00005: val_mae improved from 11016.21094 to 10578.35059, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 11983.3555 - mse: 335320064.0000 - mae: 11983.3555 - val_loss: 10578.3506 - val_mse: 236824144.0000 - val_mae: 10578.3506\n",
      "Epoch 6/100\n",
      "52/71 [====================>.........] - ETA: 0s - loss: 11497.7695 - mse: 304553152.0000 - mae: 11497.7695\n",
      "Epoch 00006: val_mae improved from 10578.35059 to 10301.73047, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 11596.0049 - mse: 318264704.0000 - mae: 11596.0049 - val_loss: 10301.7305 - val_mse: 223283504.0000 - val_mae: 10301.7305\n",
      "Epoch 7/100\n",
      "55/71 [======================>.......] - ETA: 0s - loss: 11120.7178 - mse: 281508384.0000 - mae: 11120.7178\n",
      "Epoch 00007: val_mae improved from 10301.73047 to 10052.61230, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 11205.5625 - mse: 289785248.0000 - mae: 11205.5625 - val_loss: 10052.6123 - val_mse: 219668736.0000 - val_mae: 10052.6123\n",
      "Epoch 8/100\n",
      "47/71 [==================>...........] - ETA: 0s - loss: 11417.6133 - mse: 328534336.0000 - mae: 11417.6133\n",
      "Epoch 00008: val_mae improved from 10052.61230 to 9986.48242, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 11019.8506 - mse: 296873280.0000 - mae: 11019.8506 - val_loss: 9986.4824 - val_mse: 219166208.0000 - val_mae: 9986.4824\n",
      "Epoch 9/100\n",
      "52/71 [====================>.........] - ETA: 0s - loss: 11206.0371 - mse: 316931424.0000 - mae: 11206.0371\n",
      "Epoch 00009: val_mae improved from 9986.48242 to 9888.66309, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10870.0234 - mse: 303580000.0000 - mae: 10870.0234 - val_loss: 9888.6631 - val_mse: 219652768.0000 - val_mae: 9888.6631\n",
      "Epoch 10/100\n",
      "56/71 [======================>.......] - ETA: 0s - loss: 10784.0674 - mse: 293619104.0000 - mae: 10784.0674\n",
      "Epoch 00010: val_mae improved from 9888.66309 to 9835.05176, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10975.0518 - mse: 296621024.0000 - mae: 10975.0518 - val_loss: 9835.0518 - val_mse: 219922016.0000 - val_mae: 9835.0518\n",
      "Epoch 11/100\n",
      "50/71 [====================>.........] - ETA: 0s - loss: 10818.0479 - mse: 295584736.0000 - mae: 10818.0479\n",
      "Epoch 00011: val_mae did not improve from 9835.05176\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10938.8652 - mse: 296824448.0000 - mae: 10938.8652 - val_loss: 9843.0938 - val_mse: 220172800.0000 - val_mae: 9843.0938\n",
      "Epoch 12/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 10811.4385 - mse: 286419456.0000 - mae: 10811.4385\n",
      "Epoch 00012: val_mae improved from 9835.05176 to 9799.73438, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10803.9951 - mse: 295035168.0000 - mae: 10803.9951 - val_loss: 9799.7344 - val_mse: 223298208.0000 - val_mae: 9799.7344\n",
      "Epoch 13/100\n",
      "63/71 [=========================>....] - ETA: 0s - loss: 10712.1670 - mse: 303047424.0000 - mae: 10712.1670\n",
      "Epoch 00013: val_mae improved from 9799.73438 to 9748.78906, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10721.1318 - mse: 301188288.0000 - mae: 10721.1318 - val_loss: 9748.7891 - val_mse: 220074208.0000 - val_mae: 9748.7891\n",
      "Epoch 14/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 10612.4258 - mse: 291151488.0000 - mae: 10612.4258\n",
      "Epoch 00014: val_mae improved from 9748.78906 to 9624.32324, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10574.0381 - mse: 286202720.0000 - mae: 10574.0381 - val_loss: 9624.3232 - val_mse: 223647520.0000 - val_mae: 9624.3232\n",
      "Epoch 15/100\n",
      "53/71 [=====================>........] - ETA: 0s - loss: 10555.6816 - mse: 276604512.0000 - mae: 10555.6816\n",
      "Epoch 00015: val_mae did not improve from 9624.32324\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10583.0059 - mse: 276708768.0000 - mae: 10583.0059 - val_loss: 9785.2568 - val_mse: 220280336.0000 - val_mae: 9785.2568\n",
      "Epoch 16/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 10591.0586 - mse: 292310272.0000 - mae: 10591.0586\n",
      "Epoch 00016: val_mae did not improve from 9624.32324\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10630.9766 - mse: 285292128.0000 - mae: 10630.9766 - val_loss: 9698.8945 - val_mse: 220122896.0000 - val_mae: 9698.8945\n",
      "Epoch 17/100\n",
      "53/71 [=====================>........] - ETA: 0s - loss: 10633.5645 - mse: 288244768.0000 - mae: 10633.5645\n",
      "Epoch 00017: val_mae did not improve from 9624.32324\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10655.6562 - mse: 295144896.0000 - mae: 10655.6562 - val_loss: 9695.8604 - val_mse: 220854288.0000 - val_mae: 9695.8604\n",
      "Epoch 18/100\n",
      "55/71 [======================>.......] - ETA: 0s - loss: 10502.3672 - mse: 299913312.0000 - mae: 10502.3672\n",
      "Epoch 00018: val_mae did not improve from 9624.32324\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10619.9092 - mse: 297983808.0000 - mae: 10619.9092 - val_loss: 9712.9434 - val_mse: 220785232.0000 - val_mae: 9712.9434\n",
      "Epoch 19/100\n",
      "57/71 [=======================>......] - ETA: 0s - loss: 10928.8037 - mse: 325063168.0000 - mae: 10928.8037\n",
      "Epoch 00019: val_mae did not improve from 9624.32324\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10772.7256 - mse: 311186016.0000 - mae: 10772.7256 - val_loss: 9852.5332 - val_mse: 223772112.0000 - val_mae: 9852.5332\n",
      "Epoch 20/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 10758.8652 - mse: 303328832.0000 - mae: 10758.8652\n",
      "Epoch 00020: val_mae improved from 9624.32324 to 9455.87207, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10647.3389 - mse: 293966592.0000 - mae: 10647.3389 - val_loss: 9455.8721 - val_mse: 216132208.0000 - val_mae: 9455.8721\n",
      "Epoch 21/100\n",
      "55/71 [======================>.......] - ETA: 0s - loss: 10719.7520 - mse: 292790112.0000 - mae: 10719.7520\n",
      "Epoch 00021: val_mae did not improve from 9455.87207\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10727.3535 - mse: 305463136.0000 - mae: 10727.3535 - val_loss: 9575.5322 - val_mse: 217732784.0000 - val_mae: 9575.5322\n",
      "Epoch 22/100\n",
      "55/71 [======================>.......] - ETA: 0s - loss: 10424.9727 - mse: 265082896.0000 - mae: 10424.9727\n",
      "Epoch 00022: val_mae improved from 9455.87207 to 9426.39551, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10518.0039 - mse: 271765632.0000 - mae: 10518.0039 - val_loss: 9426.3955 - val_mse: 217183696.0000 - val_mae: 9426.3955\n",
      "Epoch 23/100\n",
      "63/71 [=========================>....] - ETA: 0s - loss: 10433.5020 - mse: 290227552.0000 - mae: 10433.5020\n",
      "Epoch 00023: val_mae did not improve from 9426.39551\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10471.2217 - mse: 292810368.0000 - mae: 10471.2217 - val_loss: 9570.3154 - val_mse: 219115936.0000 - val_mae: 9570.3154\n",
      "Epoch 24/100\n",
      "57/71 [=======================>......] - ETA: 0s - loss: 10810.9033 - mse: 315745760.0000 - mae: 10810.9033\n",
      "Epoch 00024: val_mae did not improve from 9426.39551\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10680.1904 - mse: 306579712.0000 - mae: 10680.1904 - val_loss: 9450.3125 - val_mse: 214761344.0000 - val_mae: 9450.3125\n",
      "Epoch 25/100\n",
      "60/71 [========================>.....] - ETA: 0s - loss: 10340.9014 - mse: 270979616.0000 - mae: 10340.9014\n",
      "Epoch 00025: val_mae did not improve from 9426.39551\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10449.8350 - mse: 286290016.0000 - mae: 10449.8350 - val_loss: 9777.3193 - val_mse: 223065680.0000 - val_mae: 9777.3193\n",
      "Epoch 26/100\n",
      "62/71 [=========================>....] - ETA: 0s - loss: 10727.3330 - mse: 315585920.0000 - mae: 10727.3330\n",
      "Epoch 00026: val_mae improved from 9426.39551 to 9367.20801, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10560.8037 - mse: 310716736.0000 - mae: 10560.8037 - val_loss: 9367.2080 - val_mse: 211614704.0000 - val_mae: 9367.2080\n",
      "Epoch 27/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 10413.5322 - mse: 283761920.0000 - mae: 10413.5322\n",
      "Epoch 00027: val_mae improved from 9367.20801 to 9338.16895, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10501.0977 - mse: 288652544.0000 - mae: 10501.0977 - val_loss: 9338.1689 - val_mse: 209369392.0000 - val_mae: 9338.1689\n",
      "Epoch 28/100\n",
      "52/71 [====================>.........] - ETA: 0s - loss: 9867.7275 - mse: 250970896.0000 - mae: 9867.7275 \n",
      "Epoch 00028: val_mae did not improve from 9338.16895\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10447.8359 - mse: 299901088.0000 - mae: 10447.8359 - val_loss: 9560.7646 - val_mse: 214756208.0000 - val_mae: 9560.7646\n",
      "Epoch 29/100\n",
      "58/71 [=======================>......] - ETA: 0s - loss: 10106.5107 - mse: 269450976.0000 - mae: 10106.5107\n",
      "Epoch 00029: val_mae did not improve from 9338.16895\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10217.7744 - mse: 276233664.0000 - mae: 10217.7744 - val_loss: 9500.9902 - val_mse: 213404640.0000 - val_mae: 9500.9902\n",
      "Epoch 30/100\n",
      "58/71 [=======================>......] - ETA: 0s - loss: 10608.2246 - mse: 302977824.0000 - mae: 10608.2246\n",
      "Epoch 00030: val_mae did not improve from 9338.16895\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10621.8672 - mse: 295711904.0000 - mae: 10621.8672 - val_loss: 9500.8994 - val_mse: 212493360.0000 - val_mae: 9500.8994\n",
      "Epoch 31/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 10287.6670 - mse: 264573616.0000 - mae: 10287.6670\n",
      "Epoch 00031: val_mae did not improve from 9338.16895\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10292.9922 - mse: 264391936.0000 - mae: 10292.9922 - val_loss: 9465.2412 - val_mse: 211756096.0000 - val_mae: 9465.2412\n",
      "Epoch 32/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 10330.3350 - mse: 262930864.0000 - mae: 10330.3350\n",
      "Epoch 00032: val_mae improved from 9338.16895 to 9313.49805, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10364.1484 - mse: 272641216.0000 - mae: 10364.1484 - val_loss: 9313.4980 - val_mse: 210133072.0000 - val_mae: 9313.4980\n",
      "Epoch 33/100\n",
      "56/71 [======================>.......] - ETA: 0s - loss: 10571.9463 - mse: 281553824.0000 - mae: 10571.9463\n",
      "Epoch 00033: val_mae did not improve from 9313.49805\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10290.5029 - mse: 270740320.0000 - mae: 10290.5029 - val_loss: 9358.3125 - val_mse: 208224144.0000 - val_mae: 9358.3125\n",
      "Epoch 34/100\n",
      "57/71 [=======================>......] - ETA: 0s - loss: 10091.8760 - mse: 256391136.0000 - mae: 10091.8760\n",
      "Epoch 00034: val_mae improved from 9313.49805 to 9175.35352, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10138.2041 - mse: 259088240.0000 - mae: 10138.2041 - val_loss: 9175.3535 - val_mse: 205497088.0000 - val_mae: 9175.3535\n",
      "Epoch 35/100\n",
      "56/71 [======================>.......] - ETA: 0s - loss: 10328.7139 - mse: 294175520.0000 - mae: 10328.7139\n",
      "Epoch 00035: val_mae did not improve from 9175.35352\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10324.3350 - mse: 294169152.0000 - mae: 10324.3350 - val_loss: 9245.1729 - val_mse: 205016208.0000 - val_mae: 9245.1729\n",
      "Epoch 36/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 10429.0322 - mse: 284385344.0000 - mae: 10429.0322\n",
      "Epoch 00036: val_mae did not improve from 9175.35352\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10337.5156 - mse: 284122144.0000 - mae: 10337.5156 - val_loss: 9403.8320 - val_mse: 207213056.0000 - val_mae: 9403.8320\n",
      "Epoch 37/100\n",
      "62/71 [=========================>....] - ETA: 0s - loss: 10112.0566 - mse: 271359872.0000 - mae: 10112.0566\n",
      "Epoch 00037: val_mae improved from 9175.35352 to 9165.03223, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10204.0586 - mse: 270484864.0000 - mae: 10204.0586 - val_loss: 9165.0322 - val_mse: 201050816.0000 - val_mae: 9165.0322\n",
      "Epoch 38/100\n",
      "53/71 [=====================>........] - ETA: 0s - loss: 10357.8994 - mse: 285514528.0000 - mae: 10357.8994\n",
      "Epoch 00038: val_mae improved from 9165.03223 to 9058.83203, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10211.1953 - mse: 271370432.0000 - mae: 10211.1953 - val_loss: 9058.8320 - val_mse: 201379824.0000 - val_mae: 9058.8320\n",
      "Epoch 39/100\n",
      "49/71 [===================>..........] - ETA: 0s - loss: 10219.0986 - mse: 278574592.0000 - mae: 10219.0986\n",
      "Epoch 00039: val_mae improved from 9058.83203 to 9037.70020, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10310.9893 - mse: 279251744.0000 - mae: 10310.9893 - val_loss: 9037.7002 - val_mse: 198967200.0000 - val_mae: 9037.7002\n",
      "Epoch 40/100\n",
      "52/71 [====================>.........] - ETA: 0s - loss: 10283.6621 - mse: 293678688.0000 - mae: 10283.6621\n",
      "Epoch 00040: val_mae improved from 9037.70020 to 9036.05957, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9971.5986 - mse: 267464640.0000 - mae: 9971.5986 - val_loss: 9036.0596 - val_mse: 198405856.0000 - val_mae: 9036.0596\n",
      "Epoch 41/100\n",
      "49/71 [===================>..........] - ETA: 0s - loss: 10107.5732 - mse: 281408096.0000 - mae: 10107.5732\n",
      "Epoch 00041: val_mae improved from 9036.05957 to 8985.13086, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10149.8828 - mse: 276107008.0000 - mae: 10149.8828 - val_loss: 8985.1309 - val_mse: 195502512.0000 - val_mae: 8985.1309\n",
      "Epoch 42/100\n",
      "55/71 [======================>.......] - ETA: 0s - loss: 10230.3252 - mse: 291855104.0000 - mae: 10230.3252\n",
      "Epoch 00042: val_mae did not improve from 8985.13086\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10028.6045 - mse: 268672704.0000 - mae: 10028.6045 - val_loss: 9002.9756 - val_mse: 195060496.0000 - val_mae: 9002.9756\n",
      "Epoch 43/100\n",
      "56/71 [======================>.......] - ETA: 0s - loss: 10196.3955 - mse: 275100832.0000 - mae: 10196.3955\n",
      "Epoch 00043: val_mae did not improve from 8985.13086\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10197.2930 - mse: 275622496.0000 - mae: 10197.2930 - val_loss: 9032.6543 - val_mse: 194633392.0000 - val_mae: 9032.6543\n",
      "Epoch 44/100\n",
      "53/71 [=====================>........] - ETA: 0s - loss: 10363.6660 - mse: 271162336.0000 - mae: 10363.6660\n",
      "Epoch 00044: val_mae improved from 8985.13086 to 8779.35840, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 10043.9756 - mse: 258608320.0000 - mae: 10043.9756 - val_loss: 8779.3584 - val_mse: 189437808.0000 - val_mae: 8779.3584\n",
      "Epoch 45/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 9808.9844 - mse: 250872080.0000 - mae: 9808.9844\n",
      "Epoch 00045: val_mae did not improve from 8779.35840\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 9718.5420 - mse: 245592496.0000 - mae: 9718.5420 - val_loss: 8841.5166 - val_mse: 186704256.0000 - val_mae: 8841.5166\n",
      "Epoch 46/100\n",
      "58/71 [=======================>......] - ETA: 0s - loss: 10115.4492 - mse: 267145184.0000 - mae: 10115.4492\n",
      "Epoch 00046: val_mae did not improve from 8779.35840\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 10028.7383 - mse: 257364832.0000 - mae: 10028.7383 - val_loss: 8881.5205 - val_mse: 185121552.0000 - val_mae: 8881.5205\n",
      "Epoch 47/100\n",
      "51/71 [====================>.........] - ETA: 0s - loss: 10048.5801 - mse: 260550320.0000 - mae: 10048.5801\n",
      "Epoch 00047: val_mae improved from 8779.35840 to 8746.66602, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9973.7549 - mse: 252118688.0000 - mae: 9973.7549 - val_loss: 8746.6660 - val_mse: 183207984.0000 - val_mae: 8746.6660\n",
      "Epoch 48/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 9961.8066 - mse: 271514272.0000 - mae: 9961.8066  \n",
      "Epoch 00048: val_mae improved from 8746.66602 to 8686.90234, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9850.4141 - mse: 263389680.0000 - mae: 9850.4141 - val_loss: 8686.9023 - val_mse: 181119104.0000 - val_mae: 8686.9023\n",
      "Epoch 49/100\n",
      "56/71 [======================>.......] - ETA: 0s - loss: 9665.3105 - mse: 253702064.0000 - mae: 9665.3105 \n",
      "Epoch 00049: val_mae improved from 8686.90234 to 8586.82812, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9741.2041 - mse: 262684960.0000 - mae: 9741.2041 - val_loss: 8586.8281 - val_mse: 177993680.0000 - val_mae: 8586.8281\n",
      "Epoch 50/100\n",
      "56/71 [======================>.......] - ETA: 0s - loss: 9756.1074 - mse: 254713456.0000 - mae: 9756.1074  \n",
      "Epoch 00050: val_mae did not improve from 8586.82812\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 9860.7979 - mse: 253277904.0000 - mae: 9860.7979 - val_loss: 8684.8359 - val_mse: 179194976.0000 - val_mae: 8684.8359\n",
      "Epoch 51/100\n",
      "57/71 [=======================>......] - ETA: 0s - loss: 9963.1045 - mse: 269102624.0000 - mae: 9963.1045\n",
      "Epoch 00051: val_mae improved from 8586.82812 to 8441.36523, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9825.0264 - mse: 256602512.0000 - mae: 9825.0264 - val_loss: 8441.3652 - val_mse: 176947744.0000 - val_mae: 8441.3652\n",
      "Epoch 52/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 9650.3721 - mse: 244018672.0000 - mae: 9650.3721 \n",
      "Epoch 00052: val_mae improved from 8441.36523 to 8425.82520, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9641.0703 - mse: 241943696.0000 - mae: 9641.0703 - val_loss: 8425.8252 - val_mse: 174914848.0000 - val_mae: 8425.8252\n",
      "Epoch 53/100\n",
      "53/71 [=====================>........] - ETA: 0s - loss: 9476.3457 - mse: 233056432.0000 - mae: 9476.3457\n",
      "Epoch 00053: val_mae improved from 8425.82520 to 8362.52734, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 9534.4365 - mse: 232907264.0000 - mae: 9534.4365 - val_loss: 8362.5273 - val_mse: 170328336.0000 - val_mae: 8362.5273\n",
      "Epoch 54/100\n",
      "59/71 [=======================>......] - ETA: 0s - loss: 9828.7988 - mse: 255917920.0000 - mae: 9828.7988\n",
      "Epoch 00054: val_mae did not improve from 8362.52734\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 9837.3223 - mse: 252134448.0000 - mae: 9837.3223 - val_loss: 8521.2686 - val_mse: 171555888.0000 - val_mae: 8521.2686\n",
      "Epoch 55/100\n",
      "50/71 [====================>.........] - ETA: 0s - loss: 9865.4346 - mse: 283610464.0000 - mae: 9865.4346  \n",
      "Epoch 00055: val_mae improved from 8362.52734 to 8359.86523, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9637.5068 - mse: 260150240.0000 - mae: 9637.5068 - val_loss: 8359.8652 - val_mse: 167141872.0000 - val_mae: 8359.8652\n",
      "Epoch 56/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 9427.3838 - mse: 233016016.0000 - mae: 9427.3838\n",
      "Epoch 00056: val_mae improved from 8359.86523 to 8146.51660, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9490.4600 - mse: 239691968.0000 - mae: 9490.4600 - val_loss: 8146.5166 - val_mse: 166584368.0000 - val_mae: 8146.5166\n",
      "Epoch 57/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 9604.4492 - mse: 251365584.0000 - mae: 9604.4492\n",
      "Epoch 00057: val_mae did not improve from 8146.51660\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9627.0225 - mse: 250987584.0000 - mae: 9627.0225 - val_loss: 8263.9277 - val_mse: 161552560.0000 - val_mae: 8263.9277\n",
      "Epoch 58/100\n",
      "64/71 [==========================>...] - ETA: 0s - loss: 9640.4883 - mse: 256021600.0000 - mae: 9640.4883\n",
      "Epoch 00058: val_mae did not improve from 8146.51660\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9607.8896 - mse: 249083600.0000 - mae: 9607.8896 - val_loss: 8304.8877 - val_mse: 162186992.0000 - val_mae: 8304.8877\n",
      "Epoch 59/100\n",
      "62/71 [=========================>....] - ETA: 0s - loss: 9695.8984 - mse: 241551568.0000 - mae: 9695.8984 \n",
      "Epoch 00059: val_mae improved from 8146.51660 to 7959.59912, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 9680.9697 - mse: 239111168.0000 - mae: 9680.9697 - val_loss: 7959.5991 - val_mse: 154095120.0000 - val_mae: 7959.5991\n",
      "Epoch 60/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 9694.9619 - mse: 273769472.0000 - mae: 9694.9619\n",
      "Epoch 00060: val_mae improved from 7959.59912 to 7911.57812, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9624.5762 - mse: 266484176.0000 - mae: 9624.5762 - val_loss: 7911.5781 - val_mse: 157305440.0000 - val_mae: 7911.5781\n",
      "Epoch 61/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 9261.0996 - mse: 218995712.0000 - mae: 9261.0996\n",
      "Epoch 00061: val_mae did not improve from 7911.57812\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9303.7969 - mse: 220711888.0000 - mae: 9303.7969 - val_loss: 8218.2783 - val_mse: 159857232.0000 - val_mae: 8218.2783\n",
      "Epoch 62/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 9529.7910 - mse: 257043040.0000 - mae: 9529.7910 \n",
      "Epoch 00062: val_mae did not improve from 7911.57812\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9501.6973 - mse: 253423600.0000 - mae: 9501.6973 - val_loss: 8034.9141 - val_mse: 153601792.0000 - val_mae: 8034.9141\n",
      "Epoch 63/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 9323.9453 - mse: 228365904.0000 - mae: 9323.9453\n",
      "Epoch 00063: val_mae improved from 7911.57812 to 7897.57910, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9300.8447 - mse: 231806688.0000 - mae: 9300.8447 - val_loss: 7897.5791 - val_mse: 152386432.0000 - val_mae: 7897.5791\n",
      "Epoch 64/100\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 9528.1064 - mse: 246460928.0000 - mae: 9528.1064\n",
      "Epoch 00064: val_mae improved from 7897.57910 to 7849.31885, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 9575.1006 - mse: 255084160.0000 - mae: 9575.1006 - val_loss: 7849.3188 - val_mse: 148409024.0000 - val_mae: 7849.3188\n",
      "Epoch 65/100\n",
      "64/71 [==========================>...] - ETA: 0s - loss: 9510.5928 - mse: 252797504.0000 - mae: 9510.5928\n",
      "Epoch 00065: val_mae did not improve from 7849.31885\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9602.6650 - mse: 257641056.0000 - mae: 9602.6650 - val_loss: 7855.7227 - val_mse: 148758192.0000 - val_mae: 7855.7227\n",
      "Epoch 66/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 8957.4365 - mse: 210546064.0000 - mae: 8957.4365\n",
      "Epoch 00066: val_mae improved from 7849.31885 to 7672.02344, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9002.2139 - mse: 213971312.0000 - mae: 9002.2139 - val_loss: 7672.0234 - val_mse: 145509168.0000 - val_mae: 7672.0234\n",
      "Epoch 67/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 9390.3057 - mse: 245550592.0000 - mae: 9390.3057 \n",
      "Epoch 00067: val_mae improved from 7672.02344 to 7636.94238, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9373.7949 - mse: 242012304.0000 - mae: 9373.7949 - val_loss: 7636.9424 - val_mse: 146261264.0000 - val_mae: 7636.9424\n",
      "Epoch 68/100\n",
      "64/71 [==========================>...] - ETA: 0s - loss: 9290.3135 - mse: 242037792.0000 - mae: 9290.3135\n",
      "Epoch 00068: val_mae did not improve from 7636.94238\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9207.7871 - mse: 233445632.0000 - mae: 9207.7871 - val_loss: 7740.0273 - val_mse: 145660368.0000 - val_mae: 7740.0273\n",
      "Epoch 69/100\n",
      "66/71 [==========================>...] - ETA: 0s - loss: 8934.9385 - mse: 220294432.0000 - mae: 8934.9385\n",
      "Epoch 00069: val_mae improved from 7636.94238 to 7562.72705, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8954.5605 - mse: 221012256.0000 - mae: 8954.5605 - val_loss: 7562.7271 - val_mse: 140855440.0000 - val_mae: 7562.7271\n",
      "Epoch 70/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 9140.5068 - mse: 227392448.0000 - mae: 9140.5068\n",
      "Epoch 00070: val_mae improved from 7562.72705 to 7529.34766, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9167.9277 - mse: 226998320.0000 - mae: 9167.9277 - val_loss: 7529.3477 - val_mse: 138491680.0000 - val_mae: 7529.3477\n",
      "Epoch 71/100\n",
      "64/71 [==========================>...] - ETA: 0s - loss: 9002.3877 - mse: 221743696.0000 - mae: 9002.3877\n",
      "Epoch 00071: val_mae improved from 7529.34766 to 7500.32373, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9017.2441 - mse: 219664560.0000 - mae: 9017.2441 - val_loss: 7500.3237 - val_mse: 140802752.0000 - val_mae: 7500.3237\n",
      "Epoch 72/100\n",
      "66/71 [==========================>...] - ETA: 0s - loss: 9030.5029 - mse: 215251360.0000 - mae: 9030.5029\n",
      "Epoch 00072: val_mae did not improve from 7500.32373\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8993.3135 - mse: 212603136.0000 - mae: 8993.3135 - val_loss: 7602.6963 - val_mse: 140973952.0000 - val_mae: 7602.6963\n",
      "Epoch 73/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 9057.6797 - mse: 220397984.0000 - mae: 9057.6797\n",
      "Epoch 00073: val_mae did not improve from 7500.32373\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9190.5400 - mse: 235092752.0000 - mae: 9190.5400 - val_loss: 7503.5479 - val_mse: 138262880.0000 - val_mae: 7503.5479\n",
      "Epoch 74/100\n",
      "66/71 [==========================>...] - ETA: 0s - loss: 9277.2725 - mse: 236153136.0000 - mae: 9277.2725\n",
      "Epoch 00074: val_mae did not improve from 7500.32373\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9259.9971 - mse: 234955856.0000 - mae: 9259.9971 - val_loss: 7712.3286 - val_mse: 142300416.0000 - val_mae: 7712.3286\n",
      "Epoch 75/100\n",
      "63/71 [=========================>....] - ETA: 0s - loss: 9077.7695 - mse: 239011248.0000 - mae: 9077.7695\n",
      "Epoch 00075: val_mae did not improve from 7500.32373\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9122.4092 - mse: 241086480.0000 - mae: 9122.4092 - val_loss: 7582.8057 - val_mse: 138704080.0000 - val_mae: 7582.8057\n",
      "Epoch 76/100\n",
      "66/71 [==========================>...] - ETA: 0s - loss: 9351.3096 - mse: 249720352.0000 - mae: 9351.3096\n",
      "Epoch 00076: val_mae improved from 7500.32373 to 7390.55664, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9286.3701 - mse: 243611680.0000 - mae: 9286.3701 - val_loss: 7390.5566 - val_mse: 135694048.0000 - val_mae: 7390.5566\n",
      "Epoch 77/100\n",
      "64/71 [==========================>...] - ETA: 0s - loss: 9316.8770 - mse: 272130752.0000 - mae: 9316.8770\n",
      "Epoch 00077: val_mae did not improve from 7390.55664\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9244.5898 - mse: 267468000.0000 - mae: 9244.5898 - val_loss: 7622.8149 - val_mse: 138092672.0000 - val_mae: 7622.8149\n",
      "Epoch 78/100\n",
      "64/71 [==========================>...] - ETA: 0s - loss: 9160.8828 - mse: 217676160.0000 - mae: 9160.8828\n",
      "Epoch 00078: val_mae improved from 7390.55664 to 7347.51855, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9206.0908 - mse: 216895888.0000 - mae: 9206.0908 - val_loss: 7347.5186 - val_mse: 131399352.0000 - val_mae: 7347.5186\n",
      "Epoch 79/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 8968.8398 - mse: 236342384.0000 - mae: 8968.8398\n",
      "Epoch 00079: val_mae did not improve from 7347.51855\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8903.6523 - mse: 230187424.0000 - mae: 8903.6523 - val_loss: 7518.6216 - val_mse: 134031136.0000 - val_mae: 7518.6216\n",
      "Epoch 80/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 8802.0908 - mse: 205129824.0000 - mae: 8802.0908\n",
      "Epoch 00080: val_mae improved from 7347.51855 to 7250.27100, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8847.5635 - mse: 207354272.0000 - mae: 8847.5635 - val_loss: 7250.2710 - val_mse: 132016232.0000 - val_mae: 7250.2710\n",
      "Epoch 81/100\n",
      "63/71 [=========================>....] - ETA: 0s - loss: 9114.2998 - mse: 250990816.0000 - mae: 9114.2998\n",
      "Epoch 00081: val_mae did not improve from 7250.27100\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9076.4932 - mse: 247772880.0000 - mae: 9076.4932 - val_loss: 7343.9150 - val_mse: 132318416.0000 - val_mae: 7343.9150\n",
      "Epoch 82/100\n",
      "66/71 [==========================>...] - ETA: 0s - loss: 8875.1475 - mse: 219038272.0000 - mae: 8875.1475\n",
      "Epoch 00082: val_mae improved from 7250.27100 to 7235.31934, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8888.2754 - mse: 231315952.0000 - mae: 8888.2754 - val_loss: 7235.3193 - val_mse: 132733960.0000 - val_mae: 7235.3193\n",
      "Epoch 83/100\n",
      "66/71 [==========================>...] - ETA: 0s - loss: 9114.6670 - mse: 232600608.0000 - mae: 9114.6670\n",
      "Epoch 00083: val_mae did not improve from 7235.31934\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9069.6309 - mse: 227280928.0000 - mae: 9069.6309 - val_loss: 7277.2417 - val_mse: 131462200.0000 - val_mae: 7277.2417\n",
      "Epoch 84/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 9078.4609 - mse: 225253008.0000 - mae: 9078.4609\n",
      "Epoch 00084: val_mae improved from 7235.31934 to 7233.58447, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9142.7969 - mse: 231403008.0000 - mae: 9142.7969 - val_loss: 7233.5845 - val_mse: 133858536.0000 - val_mae: 7233.5845\n",
      "Epoch 85/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 8816.4775 - mse: 201711584.0000 - mae: 8816.4775\n",
      "Epoch 00085: val_mae improved from 7233.58447 to 7146.00293, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8830.0986 - mse: 202241344.0000 - mae: 8830.0986 - val_loss: 7146.0029 - val_mse: 128011392.0000 - val_mae: 7146.0029\n",
      "Epoch 86/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 8913.2832 - mse: 220487136.0000 - mae: 8913.2832\n",
      "Epoch 00086: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8920.1875 - mse: 226073072.0000 - mae: 8920.1875 - val_loss: 7227.4048 - val_mse: 127712896.0000 - val_mae: 7227.4048\n",
      "Epoch 87/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 9133.9941 - mse: 248478192.0000 - mae: 9133.9941\n",
      "Epoch 00087: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9137.2969 - mse: 248531952.0000 - mae: 9137.2969 - val_loss: 7193.7041 - val_mse: 127701264.0000 - val_mae: 7193.7041\n",
      "Epoch 88/100\n",
      "66/71 [==========================>...] - ETA: 0s - loss: 9055.6680 - mse: 223925056.0000 - mae: 9055.6680\n",
      "Epoch 00088: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9031.0283 - mse: 221789152.0000 - mae: 9031.0283 - val_loss: 7355.6846 - val_mse: 129040192.0000 - val_mae: 7355.6846\n",
      "Epoch 89/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 8798.4355 - mse: 217914752.0000 - mae: 8798.4355\n",
      "Epoch 00089: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8800.5508 - mse: 217549328.0000 - mae: 8800.5508 - val_loss: 7521.7144 - val_mse: 134289776.0000 - val_mae: 7521.7144\n",
      "Epoch 90/100\n",
      "68/71 [===========================>..] - ETA: 0s - loss: 8821.5547 - mse: 217839168.0000 - mae: 8821.5547\n",
      "Epoch 00090: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8802.0127 - mse: 215538784.0000 - mae: 8802.0127 - val_loss: 7177.0513 - val_mse: 127275696.0000 - val_mae: 7177.0513\n",
      "Epoch 91/100\n",
      "71/71 [==============================] - ETA: 0s - loss: 9078.9043 - mse: 239351392.0000 - mae: 9078.9043\n",
      "Epoch 00091: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9078.9043 - mse: 239351392.0000 - mae: 9078.9043 - val_loss: 7522.6577 - val_mse: 133216472.0000 - val_mae: 7522.6577\n",
      "Epoch 92/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 8762.2666 - mse: 207667344.0000 - mae: 8762.2666\n",
      "Epoch 00092: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8780.4111 - mse: 211925552.0000 - mae: 8780.4111 - val_loss: 7414.9736 - val_mse: 129609872.0000 - val_mae: 7414.9736\n",
      "Epoch 93/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 9080.1572 - mse: 240277264.0000 - mae: 9080.1572\n",
      "Epoch 00093: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8987.9355 - mse: 234517664.0000 - mae: 8987.9355 - val_loss: 7282.2627 - val_mse: 127945408.0000 - val_mae: 7282.2627\n",
      "Epoch 94/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 8969.2881 - mse: 229284928.0000 - mae: 8969.2881\n",
      "Epoch 00094: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9004.9873 - mse: 233724688.0000 - mae: 9004.9873 - val_loss: 7155.3301 - val_mse: 124900088.0000 - val_mae: 7155.3301\n",
      "Epoch 95/100\n",
      "69/71 [============================>.] - ETA: 0s - loss: 8539.2559 - mse: 200601984.0000 - mae: 8539.2559\n",
      "Epoch 00095: val_mae did not improve from 7146.00293\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8607.1699 - mse: 206918432.0000 - mae: 8607.1699 - val_loss: 7513.9072 - val_mse: 132611296.0000 - val_mae: 7513.9072\n",
      "Epoch 96/100\n",
      "70/71 [============================>.] - ETA: 0s - loss: 8729.6494 - mse: 207707056.0000 - mae: 8729.6494\n",
      "Epoch 00096: val_mae improved from 7146.00293 to 7042.73877, saving model to .\\modelo_mlp_regression.hdf5\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8725.7715 - mse: 207619632.0000 - mae: 8725.7715 - val_loss: 7042.7388 - val_mse: 123464168.0000 - val_mae: 7042.7388\n",
      "Epoch 97/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 8711.5352 - mse: 201653024.0000 - mae: 8711.5352\n",
      "Epoch 00097: val_mae did not improve from 7042.73877\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8629.5889 - mse: 198116240.0000 - mae: 8629.5889 - val_loss: 7147.4116 - val_mse: 123686888.0000 - val_mae: 7147.4116\n",
      "Epoch 98/100\n",
      "67/71 [===========================>..] - ETA: 0s - loss: 8816.1143 - mse: 229493280.0000 - mae: 8816.1143\n",
      "Epoch 00098: val_mae did not improve from 7042.73877\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8878.0234 - mse: 234004384.0000 - mae: 8878.0234 - val_loss: 7143.6372 - val_mse: 124944280.0000 - val_mae: 7143.6372\n",
      "Epoch 99/100\n",
      "66/71 [==========================>...] - ETA: 0s - loss: 9174.1924 - mse: 248728432.0000 - mae: 9174.1924\n",
      "Epoch 00099: val_mae did not improve from 7042.73877\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 9082.2451 - mse: 246341120.0000 - mae: 9082.2451 - val_loss: 7213.0273 - val_mse: 125913392.0000 - val_mae: 7213.0273\n",
      "Epoch 100/100\n",
      "65/71 [==========================>...] - ETA: 0s - loss: 8776.3535 - mse: 208577856.0000 - mae: 8776.3535\n",
      "Epoch 00100: val_mae did not improve from 7042.73877\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 8748.9189 - mse: 209591888.0000 - mae: 8748.9189 - val_loss: 7136.0269 - val_mse: 124506408.0000 - val_mae: 7136.0269\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='./modelo_mlp_regression.hdf5', verbose=1, save_best_only=True, monitor='val_mae')\n",
    "\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mse', 'mae'])\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_test, y_test), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5655492-ea6e-45d1-b748-55eaebae088b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAE/CAYAAABGqgvRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABfNElEQVR4nO3deZxT1f3/8de92SfJ7BubIIggoqDigqB8oVWQARfctWK/aq1+rdu3pVJF3EpVSkVbv9rWn7XW2lpsFcUialVUBBdwB1RE9oHZt8xkz/39EYiOMDAgMEl8Px8PHjI3NzfnkwzHd05OzjEsy7IQEREREclCZlc3QERERERkX1HYFREREZGspbArIiIiIllLYVdEREREspbCroiIiIhkLYVdEREREclaCrtp6Je//CWnnXYap512GoMHD2bs2LGpn0OhUKev86Mf/Ygvvvhip+fcd999zJ0791u2eO/5+OOPGTNmzF651u9+9ztuv/12oOPnYsGCBVx00UW7vNb999/Pf/7zH2DvPmdvv/02AwYM4Oc///l2t1100UUcccQRqZ8XLlzIueeey6mnnkpFRQXXXnstW7ZsAWDjxo0ccsghqd+Tr/+JRCJ7pa0isnPqu787ffdTTz3Fj3/8471yLdn37F3dANnetGnTUn8fM2YMs2bN4rDDDtvt6zz00EO7POfaa6/d7etmos48Fzvz9ttvc9BBBwF7/zkrKSlh4cKFBINBPB4PAJs2bWLNmjWpc6qqqrjhhht46qmn6NGjBwAPPvgg1113HU888QQAbrebZ555Zq+2TUQ6T3333pfOfbdkDoXdDPO73/2ODz74gOrqagYMGMDUqVOZPn06dXV11NTU0KNHD+69916KiooYM2YM9913H21tbcyePZtevXqxatUqIpEI06dP57jjjmPq1Kn079+fSy+9lMMOO4zLL7+cN998k+rqaiZPnswPf/hD4vE4M2fO5JVXXsHv93P44YezevVqHnvssXZta2tr49Zbb2Xt2rU0NTXh9XqZNWsWffv25aKLLmLo0KG89957bN68maOOOoq7774b0zT529/+xqOPPorP5+Pggw/eYd333HMPgUCA6dOnA/D666/zu9/9jieffJLf//73/Oc//yEcDhMMBrnhhhs46aST2t1/23Nx2GGHcd999zFv3jzy8/Pp3bt36pw1a9Zw++2309bWRnV1NQMHDuTee+/ln//8J5988gkzZ87EZrPx8ssvp56zpUuXMnPmTILBIA6Hg+uuu44TTzyRp556ipdeegnTNFm3bh0Oh4O77757h/Xl5+fTq1cv/vOf/zBx4kQA5s6dy8SJE1NBtqGhgWg0SltbW+p+F198MYcccsge/BaJyP6mvjv7+u5ttmzZwq233sqmTZuwLIvTTz+dyy67jFgsxh133MF7772Hw+GgZ8+e3Hnnnbhcrh0e93q9e/rrJbuwX6YxBAIBJkyYwMaNG3d63muvvcbEiROZOHEiP/3pT2ltbd0fzcs4mzZt4umnn2bWrFn8+9//ZujQofzjH//g5Zdf7nB076OPPuKSSy5h7ty5nHXWWdx///3bnROJRCgoKOCJJ57gt7/9Lb/5zW8Ih8M8+eSTLF++nOeee44nnniCDRs27LBdr7/+Orm5ucyZM4cXXniBwYMH8/jjj6duX79+PY899hjPPvssb731Fu+88w4rV67k/vvv569//Sv/+te/cDgcO7z22Wefzfz581MfyT/11FOcc845bNq0icWLF/PXv/6VefPmcf311/Pb3/62w+fuP//5Dy+++CJz587liSeeIBAIpG6bM2cOp59+Ov/4xz948cUX2bhxIwsXLuTCCy9k8ODB/PznP2/XETc0NHDNNddw0003MW/ePO6++26mTJmSen7effddbr75Zp577jmOPPJIHn744Q7bdfrpp7d73Z5//nkmTJiQ+nngwIGcc845nHHGGYwfP55p06bx6quvMnLkyNQ5oVBouykMt912W4ePKSL7l/ru7Ou7AX72s59x7LHHMm/ePP7+97/z7LPP8u9//5sPPviAd955h2effZannnqKXr168dlnn3V4XPadfR52P/zwQ84//3zWrl270/Oam5uZOnUqs2fPZt68eQwcOJDZs2fv6+ZlpKFDh2K3JwflL774Yo488kgeeeQRbr31VlatWtVu9G+b7t27p0YBBw0aRFNT0w6v/b3vfQ+AQw89lEgkQltbG6+99hqnnXYaLpcLp9PJueeeu8P7jhs3jjPOOIPHHnuMX/7yl7zzzjvt2jJ69GhM08Tn89G7d2+amppYsmQJI0aMoKSkBKDDa/fq1YuBAwfyyiuvpO43fvx4evTowd133828efOYNWsWTzzxxE7fJC1ZsoSTTjoJn8+H3W7nzDPPTN02ZcoUCgsLeeihh7j11luprq7e4XO5zUcffcQBBxzAkCFDAOjfvz9HHnkk77zzTuo5LC8vB3b+nG97bpYvX05dXR3Lli2jb9++5OXltTtn6tSpvPHGG1x77bW43W5mzpzJD37wA+LxOPDVNIav/7nllls6fEwR2b/Ud2df393W1sZ7773HhRdeCIDf72fSpEm8/vrrHHzwwdhsNs4++2zuvfdexo4dy5FHHtnhcdl39nnYnTNnDrfccgulpaWpY3PnzuWMM87gtNNO48YbbyQcDrN27Vq6d++emlszevTo1KRyaS8nJyf191//+tfcd999FBQUcO655zJixAgsy9ruPm63O/V3wzB2eA6Ay+VKnQNgWVaqc97GNHf8a/O3v/2Nm266CbfbzcSJE5kwYUK7x9lRG77ZFpvN1mHdZ599NnPnzuW5557jpJNOwuv1snz5cs477zwCgQAjRozgsssu6/D+O6r964/3v//7v8yZM4cePXrwwx/+kEMPPbTD5wkgkUhsd8yyLGKxWIf1dsTpdHLyySfz3HPPpf59fN3LL7/Mv/71LwoKChg7dizTpk1j/vz5rF69mhUrVuy0ZhFJD+q7s6/vTiQS292eSCSIxWLk5ubyzDPPcMMNN2Cz2bjuuuv485//3OFx2Xf2edidMWMGw4YNS/28atUq5syZwxNPPMEzzzxDUVERDz/8MH369GHLli18+umnQPJj3Nra2n3dvIy3aNEiLr74Yk4//XSKiopYvHhxaqRvbxk1ahTPPvsskUiEWCzG008/3WFbzjjjDM4++2wOPPBAXnnllV225fjjj+fNN99MrSrQ0bUBTjrpJJYvX86cOXM455xzgOTHTYMHD+a///u/OeaYY3j55Zd3+pgnnHACCxYsoLm5mUQi0e5jw0WLFnHVVVcxfvx4DMPgww8/TF3LZrOlOsJthgwZwpo1a/joo4+A5O/2u+++yzHHHLPTmjty+umn8/TTT/Puu+9ywgkntLvN6/Vyzz33tPtW8saNG3G5XBxwwAF79Hgi0nXUd2dH3+3z+RgyZEhq2kdLSwtz587l+OOP59VXX+WHP/whRxxxBFdffTWnn346n376aYfHZd/Z719Qe/vtt1m3bl3qFz4ajTJo0CByc3O5++67ufnmm0kkEpxzzjkdzgGSr1x11VXMnDmTBx54AJvNxpFHHsn69ev36mNMmjSJNWvWcPrpp5OTk0PPnj1TqwZ83SWXXML06dN56qmnsNlsHHrooXz++ec7vfaAAQOYMmUKF198MV6vl8MPP7zDc51OJ+PHj2fx4sWp8yZMmMCLL77I+PHjcTgcDB8+nKampnbzub5u1KhRfPbZZ5x55pnk5uYycOBAGhoaALj++uu56qqryMvLw+PxcPTRR6eey9GjR3P33XcTjUZT1yosLOS+++7jjjvuIBQKYRgGd955JwceeCDvv//+zp/UHTjiiCMIBoOMGTNmuxGZ4447jptvvpkbbriBlpYWbDYbJSUlPPDAA+Tl5dHS0pKas/tNd911l77IJpJm1HdnT989a9Ysbr/9dp566ikikQgTJ05k0qRJJBIJXn/9dSZMmEBOTg55eXnccccddOvWbYfHZd8xrJ2Nz+9FY8aM4S9/+Qsvv/wyGzZsSC3R0traSjwex+v1smbNmtQ0huXLl/OLX/yCZ599dn80T3Zi0aJF1NXVpYLUL3/5S1wuF1OmTOnilomISEfUd4sk7fdNJY499lheeukl6urqsCyLW2+9lUcffRTDMLjkkkuoqqrCsiz+9Kc/MX78+P3dPNmB/v37M3fu3NRmBg0NDVxxxRVd3SwREdkJ9d0iSZ0a2X3llVe4//77aWtrY+TIke0Wzu6sbSO7PXv25Mknn+TRRx8lkUhwyCGH8Ktf/QqXy8XChQv5zW9+QyQSYfjw4dx0002ayiAiGSsQCHDeeefx+9//np49e7J48WLuvPNOwuEwp5xyCtdffz0AK1euZNq0aQQCAYYNG8Ztt92G3W6nsrKSKVOmUFdXx4EHHsisWbPwer00Nzfzs5/9jA0bNlBYWMi9995LSUkJkUiEm266iU8++QS3282sWbPo169fFz8LIiJdzNqF9evXWyNHjrQ2b95sRSIR6/zzz7cWLly4q7uJiHynffDBB9aECROsQw891NqwYYMVDAatUaNGWevXr7ei0ah1ySWXpPrSiooK6/3337csy7J+8YtfWI8//rhlWZZ1+eWXW88995xlWZZ1//33WzNnzrQsy7Juu+026w9/+INlWZb19NNPW9dee61lWZb1//7f/7Nuvvlmy7Is65133rHOOuus/VWuiEja2uU0hpdeeonx48dTXl6Ow+Fg9uzZqbXpRERkx7657OJHH31E79696dWrF3a7nYkTJ7JgwQI2bdpEKBRi6NChQPJLRQsWLCAajfLuu+8yduzYdscBFi5cmNptb8KECbz++utEo1EWLlzIqaeeCsDRRx9NQ0MDlZWV+7lyEZH0ssvVGLZtl3fppZdSU1PD6NGjue666/ZD00REMteMGTPa/VxdXZ1agB+gtLSUqqqq7Y6XlJRQVVVFQ0NDagH9rx//5rXsdjs+n4/6+vodXmvLli107959n9UpIpLudjmyG4/HWbJkCb/+9a+ZM2cOH3/88U7X0xMRke1ZO/h6REcL1u/seEc62jCgo+MiIt8VuxzZLS4uZvjw4RQWFgLJLQk/+ugjJk2a1KkHaGhoJZHY9epmRUU+6up2vL5eNlB9mU31Za49qc00DQoKvHu1HWVlZe02yqmurqa0tHS74zU1NZSWllJYWEggECAej2Oz2VLHITkqXFtbS3l5ObFYjEAgQH5+PqWlpdTU1NC7d+9219od6rOTVF9mU32ZbW/327sMu6NHj+aGG26gubkZr9fLG2+8kdqDuzMSCatTHee2c7OZ6stsqi9zpUNt23ZtWrduHT179uS5557jzDPPpEePHrhcLpYtW8ZRRx3F3LlzOfHEE3E4HAwbNoz58+czceLE1HFILrA/d+5crrjiCubPn8+wYcNwOByMGjWKZ555hmHDhrF06VJcLtduT2FQn/0V1ZfZVF9m25v17TLsDhkyhMsuu4wLLriAaDTKiBEjOPPMM/daA0REvgtcLhd33XUXV199NeFwmFGjRjFu3DgguQPTtGnTaG1tZdCgQUyePBmAW265halTp/Lggw/SrVs37rnnHgCuvfZapk6dSkVFBX6/n1mzZgFw0UUXMX36dCoqKnA6ncycObNrihURSSP7fAe1urpAp9J5SYmfmpqWfdmULqX6Mpvqy1x7UptpGhQV+fZRi9Kb+uwk1ZfZVF9m29v99i5HdkVERESykWVZBAJNBIMBEol4Vzen06qrTRKJRFc3Y5/ZWX12u5OCghJsts5HWIVdERER+U5qaKjBMAwKC8uw2ew7XfEkndjtJrFY9obdjuqzLIvW1mYaGmooLu7W6etpTRoRERH5TopEQuTnF2G3OzIm6H6XGYaB15tLLBbZrfsp7IqIiMh3lIVhKAplkj15U6JXWERERESylubsioiIiKSB3/zmbj7++ENisSgbN26gT5++AJx99nlUVJzaqWv88IcX8Oc//63D2xcteo1PP13JZZdd8a3aOmPGrRxxxFGMHz/xW11nf1DYFREREUkDP/3pDQBs3lzJ1Vf/eKehtSO7us/IkaMYOXLUHrUvUynsioiIiKS5s86ayKBBg1m16jP+8Ic/8fe/P86yZe/S3NxMfn4+M2bMpKiomJEjh7Fo0VIefvgP1NbWsGHDeqqqtjBhwmlcfPGlzJ8/j/ffX8ZNN93KWWdNZOzY8bzzzhKCwRDTpt3GwIGH8OWXXzBjxm3E43GGDBnKW28t5h//mNth2/7972d54om/YhgGAwYcwvXX/xyn08mdd97Gl1+uBuCMM87m1FPP4MUXF/C3v/0F0zTp3r07N998By6Xa58+dwq7IiIiIsCbH29m0Ueb98m1Rx7ejRGHdX65rB057rjjuf32O9m8eSPr16/l97//E6Zpcscd03nxxQWcf/4P2p3/xRereOCB/0cg0MI555zOpEnnbHfNvLw8HnroL/zzn0/w2GN/YsaMX/PLX97Kj350BcOHj+Qf/3iceLzjNYhXr/6Cv/zlT/zxj38mLy+f3/zmbh555CGOP34kzc3NPPLI32hqauT+++/l1FPP4KGHHuSPf3yEgoJC/vjHB1i/fi39+w/4Vs/LrugLaiIiIiIZYNCgwQD06nUAP/nJ9cybN5ff/W42y5d/TDDYtt35Rx45DIfDQUFBIbm5ubS2BrY759hjjwegb9+DaG5uprm5iS1bNjN8+EgAKipO22mbPvhgGSNGnEBeXj4Ap556BsuWvUPfvv1Yv34d//u/P+GFF57nyiuvBmDEiBO48spL+b//u4/jjz9hnwdd0MiuiIiICAAjDvv2o6/70raP+z/9dAXTpt3IeeddwOjR38NmM7Gs7bf5djqdqb8bhrHLcyzLwjRtOzyvI9tvL24Rj8fJy8vnscfm8O67b7NkyZtccskPeOyxOVx33c/44ovTWLJkEXfccTOXXHI5Y8eO7/Tj7QmN7IqIiIhkkPfee48jjjiK008/iz59+vLOO2/vte2DfT4fPXv2ZMmSNwF46aUFO13b9ogjjmLRotdpbm4C4Nln53LEEcNYtOg1br/9Zo4/fiTXXfczPB4P1dVVnHfeGeTn53PRRf/NuHEVfP75Z3ul3TujkV0RERGRDPL975/MDTf8lIsvPg+bzU6/fgexeXPlXrv+TTfdxp133s5DDz1Av379d/oFsoMO6s9FF/03P/nJ5cRiMQYMOIQpU36B0+ni1Vdf5qKLzsHpdDJq1Bj69TuISy/9Mddd9z+4XG58Pj/Tpt2619rdEcPanbHqPVBXF9jBEPf2Skr81NS07MumdCnVl9lUX+bak9pM06CoyLePWpTe1Gcnqb7M1tn6tmxZR3l57/3Qor3LbjeJxfbOSO6OPPLIQ0yceAbFxcW89torvPji88yY8et99njftKv6dvS67azf1siuiIiIiKSUlZVz/fX/g91ux+/PZerUm7u6Sd+Kwq6IiIiIpIwfPzEjdkbrLH1BTURERESylsKuiIiIiGQthV0RERERyVoKuyIiIiKStRR2RURERCRrKeyKiIiIpIH/+Z/LeOmlBe2OBYNBxo//Ho2NjTu8z4wZtzJ//jxqa2v42c+u2eE5I0cO2+njVlZu4s47bweSWxHfddcdu9/4b3j44T/w8MN/+NbX2RsUdkVERETSwPjxE3nppRfaHXvttVc48sijyM/P3+l9i4tLmDXrt3v0uFu2bGbTpo0ADBw4KOPX1f0mrbMrIiIiAjQvfpOmRa/vk2vnjTyR3ONH7PScMWNO4v/+7z6am5vIzc0D4IUX5nPOORfw/vvL+OMfHyAcDtHS0sKVV17DmDHfT9138+ZKrr76x/zzn/PYvLmS22+/mWAwyKGHDk6dU1NTzZ133kEg0EJdXS3f//5Yrrzyau67bxaVlZv4zW/uZvTo7/GnP/2R++//I+vXr2PmzBm0tDTjdnu47rqfccghhzJjxq14vT4++2wlNTXV/Pd//4iKilM7rOvNN9/goYcexLISdO/egylTbqSwsIj777+Xd999G5vNZOTIUVxyyeUsXfoODz74W8DA7/dz662/2mXQ3xWN7IqIiIikgZycHE44YRSvvPIfAGpra1i/fh3HHjucf/3rH0ydejN/+tPj3HTTdP7854c6vM7s2TMZP34if/7z3zjssCGp4y+99AInnTSWP/7xzzz66BM8/fQ/aWxs5Nprf8aAAYfw05/e0O46d9xxM2effR6PPvoEV1/9v0ybdgORSASA6uoqHnjg/3H33bP5v/+7r8O2NDTU8+tf/4o775zFo48+wWGHDeGee2ayZctm3nprMY8++ncefPBPbNy4gXA4zKOPPswNN9zEww8/xogRJ/L5559+m6cU0MiuiIiICAC5x4/Y5ejrvlZRcSoPPfQgp59+Ji+++Dxjx47HNE1uvvkOFi9+g1df/Q8rVnxCMBjs8Brvv7+MW2+dAcDJJ5+SmoN7wQUX8d57S/nb3x5jzZrVxGJRQqEdX6etrY2NGzcyatQYAAYPPozc3FzWr18HwDHHHIthGPTt24/m5qYO27JixXIOOeRQunXrDsCpp07iscf+THFxCS6XiyuvvITjjz+BH/3oSlwuFyNHnsgNN/yUE04YxQknjOLoo4/b/SfxGzSyKyIiIpImhgw5grq6WqqqtvDCC8+npgdcddWPWLlyOQMGDOSHP7wUy7J2chWDRCJ5u2EYmGYy7v3ud7N58sknKC/vxsUXX0peXn6H17GsxHa3WRbE43EAnE5X6vo7Y1mJb/xsEY/Hsdvt/PGPf+ayy66kqamJK674b9avX8e5517IAw/8kZ49e/HAA7/l0Ucf3un1O0NhV0RERCSNnHLKBB599GFyc3Pp0aMnzc1NbNiwjksvvYLhw0fy9ttLSCQSHd5/2LBjeOGF+UDyC27bph4sXfo2F1xwEWPGfJ/q6ipqaqpJJBLYbPZUiN3G6/XRo0dPXnvtFQA++eRj6uvr6Nu3327VMmjQYFas+JjNmysBePbZpzjyyKP4/PNP+clPLmfIkCP4yU+uo0+fvqxfv44f/ehi2traOOecCzjnnAs0jUFEREQk24wbV8HZZ5/KL34xHYDc3DwmTDidiy46B6/Xy2GHDSEUCnU4leF///fn3HHHdJ599ikGDhxETo4XgB/84Ifcccd0fD4/hYWFDBw4iMrKTRx88AACgRbuuONmKipOS11n+vQ7+PWvf8XDD/8Bh8PJjBkzcTgcu1VLYWERU6bcxI03/oxoNEZ5eTlTp06nuLiYwYMPZ/Lkc3G73fTvP4Djjjset9vNHXfcgmnacLlcTJnyiz18Fr9iWDsfB//W6uoCqaH0nSkp8VNT07Ivm9KlVF9mU32Za09qM02DoiLfPmpRelOfnaT6Mltn69uyZR3l5b33Q4v2LrvdJBbreGQ30+2qvh29bjvrtzWNQURERESylsKuiIiIiGQthV0RERH5jjK2Wy1A0tuezL5V2BUREZHvJKfTTWNjLbFYdI9ClOxflmXR2tqM3e7crftpNQYRERH5TiooKCEQaKK+vopEIr7rO6QJ0zR3uvRYpttZfXa7k4KCkt26nsKuiIiIfCcZhoHfn4/fn9/VTdktWk1j92gag4iIiIhkrU6N7E6ePJm6ujrs9uTpt99+O0OGDNmnDRMRERER+bZ2GXYty+LLL79k4cKFqbArIiIiIpIJdjmN4csvv8QwDH70ox9x6qmn8te//nV/tEtERERE5Fvb5VBtc3Mzw4cP59ZbbyUUCjF58mQOPPBARowY0akH2J0tN0tK/J0+NxOpvsym+jJXNtcmIiI7t8uwe8QRR3DEEUcAkJOTw1lnncVrr73W6bCrfdaTVF9mU32Za09q29ke6yIikll2OY1h6dKlLFmyJPWzZVmauysiIiIiGWGXYbelpYWZM2cSDocJBAI8/fTTnHTSSfujbSIiIiIi38ouh2hHjx7Nhx9+yOmnn04ikeCCCy5ITWsQEREREUlnnZqPcN1113Hdddft46aIiIiIiOxd2kFNRGQ/euaZZ6ioqKCiooK7774bgJUrV3LmmWcyduxYbrrpJmKxGACVlZVceOGFjBs3jiuvvJLW1lYguUrO5ZdfzimnnMKFF15ITU0NAJFIhClTpnDKKadwxhlnsHr16q4pUkQkjSjsiojsJ8FgkBkzZvDYY4/xzDPPsHTpUhYvXsyUKVO4+eabeeGFF7Asizlz5gBw2223ccEFF7BgwQIGDx7MAw88AMC9997LsGHDeP755zn77LOZMWMGAI899hgej4fnn3+eG2+8kalTp3ZZrSIi6UJhV0RkP4nH4yQSCYLBILFYjFgsht1uJxQKMXToUAAmTZrEggULiEajvPvuu4wdO7bdcYCFCxcyceJEACZMmMDrr79ONBpl4cKFnHrqqQAcffTRNDQ0UFlZuf8LFRFJI1pDTERkP/H5fFx77bWccsopuN1ujjnmGBwOByUlJalzSkpKqKqqoqGhAZ/Pl1rqcdtxgOrq6tR97HY7Pp+P+vr6dse33WfLli107969023URkBfUX2ZTfVltr1Zn8KuiMh+8umnn/Kvf/2LV199Fb/fz89+9jPefPPN7c4zDAPL2n4zHsMwOry2ae74g7qOjndEGwElqb7Mpvoy297eDEjTGERE9pNFixYxfPhwioqKcDqdTJo0ibfffpva2trUOTU1NZSWllJYWEggECAej7c7DlBaWpq6TywWIxAIkJ+fT2lpaerLat+8j4jId5XCrojIfjJw4EAWL15MW1sblmXxyiuvcMwxx+ByuVi2bBkAc+fO5cQTT8ThcDBs2DDmz5/f7jjAqFGjmDt3LgDz589n2LBhOBwORo0axTPPPAMkd790uVy7NYVBRCQbaRqDiMh+MnLkSFasWMGkSZNwOBwcdthhXH755Zx00klMmzaN1tZWBg0axOTJkwG45ZZbmDp1Kg8++CDdunXjnnvuAeDaa69l6tSpVFRU4Pf7mTVrFgAXXXQR06dPp6KiAqfTycyZM7usVhGRdGFYO5oYthdp/leS6stsqi9z7e25X9lOfXaS6stsqi+zac6uiIiIiEgnKeyKiIiISNZS2BURERGRrKWwKyIiIiJZS2FXRERERLKWwq6IiIiIZC2FXRERERHJWgq7IiIiIpK1FHZFREREJGsp7IqIiIhI1lLYFREREZGspbArIiIiIllLYVdEREREspbCroiIiIhkLYVdEREREclaCrsiIiIikrUUdkVEREQkaynsioiIiEjWUtgVERERkaylsCsiIiIiWUthV0RERESylsKuiIiIiGQthV0RERERyVoKuyIiIiKStRR2RURERCRrKeyKiIiISNZS2BURERGRrNXpsHv33XczderUfdkWEREREZG9qlNhd8mSJTz99NP7ui0iIiIiInvVLsNuY2Mjs2fP5oorrtgf7RERERER2Wvsuzph+vTpXH/99WzevHmPHqCoyNfpc0tK/Hv0GJlC9WU21Ze5srk2ERHZuZ2G3SeffJJu3boxfPhwnnrqqT16gLq6AImEtcvzSkr81NS07NFjZALVl9lUX+bak9pM09itN+oiIpK+dhp258+fT01NDaeddhpNTU20tbXxq1/9ihtvvHF/tU9EREREZI/tNOw+8sgjqb8/9dRTvPPOOwq6IiIiIpIxtM6uiIiIiGStXX5BbZtJkyYxadKkfdkWEREREZG9SiO7IiIiIpK1FHZFREREJGsp7IqIiIhI1lLYFREREZGspbArIiIiIllLYVdEREREspbCroiIiIhkLYVdEREREclaCrsiIiIikrUUdkVE9qNXXnmFSZMmMW7cOH75y18CsHjxYiZOnMjJJ5/M7NmzU+euXLmSM888k7Fjx3LTTTcRi8UAqKys5MILL2TcuHFceeWVtLa2AtDc3Mzll1/OKaecwoUXXkhNTc3+L1BEJM0o7IqI7CcbNmzglltu4YEHHmDevHmsWLGC1157jRtvvJEHHniA+fPn88knn/Daa68BMGXKFG6++WZeeOEFLMtizpw5ANx2221ccMEFLFiwgMGDB/PAAw8AcO+99zJs2DCef/55zj77bGbMmNFltYqIpAuFXRGR/eSll15i/PjxlJeX43A4mD17Nh6Ph969e9OrVy/sdjsTJ05kwYIFbNq0iVAoxNChQwGYNGkSCxYsIBqN8u677zJ27Nh2xwEWLlzIxIkTAZgwYQKvv/460Wi0S2oVEUkX9q5ugIjId8W6detwOBxceuml1NTUMHr0aPr3709JSUnqnNLSUqqqqqiurm53vKSkhKqqKhoaGvD5fNjt9nbHgXb3sdvt+Hw+6uvrKSsr63Qbi4p8nT63pMTf6XMzkerLbKovs+3N+hR2RUT2k3g8ztKlS3nsscfIycnhf/7nf/B4PNudZxgGlmXt1vGOmObufYBXVxcgkdj+Mb6ppMRPTU3Lbl07k6i+zKb6Mtue1GeaRodv1jWNQURkPykuLmb48OEUFhbidrv53ve+x5tvvkltbW3qnOrqakpLSykrK2t3vKamhtLSUgoLCwkEAsTj8XbHITkqvO0+sViMQCBAfn7+/itQRCQNKeyKiOwno0ePZtGiRTQ3NxOPx3njjTcYN24ca9asYd26dcTjcZ577jlOPPFEevTogcvlYtmyZQDMnTuXE088EYfDwbBhw5g/f3674wCjRo1i7ty5AMyfP59hw4bhcDi6pFYRkXShaQwiIvvJkCFDuOyyy7jggguIRqOMGDGC888/n759+3L11VcTDocZNWoU48aNA2DWrFlMmzaN1tZWBg0axOTJkwG45ZZbmDp1Kg8++CDdunXjnnvuAeDaa69l6tSpVFRU4Pf7mTVrVpfVKiKSLgxrRxPA9iLN/0pSfZlN9WWuvT33K9upz05SfZlN9WU2zdkVEREREekkhV0RERERyVoKuyIiIiKStRR2RURERCRrKeyKiIiISNZS2BURERGRrKWwKyIiIiJZS2FXRERERLKWwq6IiIiIZC2FXRERERHJWgq7IiIiIpK1FHZFREREJGsp7IqIiIhI1lLYFREREZGspbArIiIiIllLYVdEREREspbCroiIiIhkLYVdEREREclanQq79913H+PHj6eiooJHHnlkX7dJRERERGSvsO/qhHfeeYe33nqLZ599llgsxvjx4xk1ahR9+/bdH+0TEREREdljuxzZPeaYY/jLX/6C3W6nrq6OeDxOTk7O/mibiIiIiMi3ssuRXQCHw8Fvf/tb/vSnPzFu3DjKyso6/QBFRb5On1tS4u/0uZlI9WU21Ze5srk2ERHZuU6FXYBrrrmGH/3oR1xxxRXMmTOHc889t1P3q6sLkEhYuzyvpMRPTU1LZ5uTcVRfZlN9mWtPajNNY7feqIuISPra5TSG1atXs3LlSgA8Hg8nn3wyn3322T5vmIiIiIjIt7XLsLtx40amTZtGJBIhEonw8ssvc9RRR+2PtomIiIiIfCu7nMYwatQoPvzwQ04//XRsNhsnn3wyFRUV+6NtIiIiIiLfSqfm7F5zzTVcc801+7otIiIiIiJ7lXZQExEREZGspbArIiIiIllLYVdEREREspbCroiIiIhkLYVdEREREclaCrsiIiIikrUUdkVEREQkaynsioiIiEjWUtgVERERkaylsCsiIiIiWUthV0RERESylsKuiIiIiGQthV0RERERyVoKuyIiIiKStRR2RURERCRrKeyKiIiISNZS2BURERGRrKWwKyIiIiJZS2FXRKQL3H333UydOhWAlStXcuaZZzJ27FhuuukmYrEYAJWVlVx44YWMGzeOK6+8ktbWVgCam5u5/PLLOeWUU7jwwgupqakBIBKJMGXKFE455RTOOOMMVq9e3TXFiYikEYVdEZH9bMmSJTz99NOpn6dMmcLNN9/MCy+8gGVZzJkzB4DbbruNCy64gAULFjB48GAeeOABAO69916GDRvG888/z9lnn82MGTMAeOyxx/B4PDz//PPceOONqTAtIvJdprArIrIfNTY2Mnv2bK644goANm3aRCgUYujQoQBMmjSJBQsWEI1Geffddxk7dmy74wALFy5k4sSJAEyYMIHXX3+daDTKwoULOfXUUwE4+uijaWhooLKycj9XKCKSXhR2RUT2o+nTp3P99deTm5sLQHV1NSUlJanbS0pKqKqqoqGhAZ/Ph91ub3f8m/ex2+34fD7q6+t3eK0tW7bsr9JERNKSvasbICLyXfHkk0/SrVs3hg8fzlNPPQWAZVnbnWcYRofHO2KaOx676Oh4R4qKfJ0+t6TEv1vXzjSqL7Opvsy2N+tT2BUR2U/mz59PTU0Np512Gk1NTbS1tWEYBrW1talzampqKC0tpbCwkEAgQDwex2azpY4DlJaWUltbS3l5ObFYjEAgQH5+PqWlpdTU1NC7d+9219oddXUBEontg/Y3lZT4qalp2a1rZxLVl9lUX2bbk/pM0+jwzbqmMYiI7CePPPIIzz33HM888wzXXHMNY8aM4c4778TlcrFs2TIA5s6dy4knnojD4WDYsGHMnz+/3XGAUaNGMXfuXCAZoIcNG4bD4WDUqFE888wzACxduhSXy0X37t33f6EiImlEYVdEpIvNmjWLO++8k1NOOYVgMMjkyZMBuOWWW5gzZw7jx49n6dKlXHfddQBce+21fPDBB1RUVPC3v/2N6dOnA3DRRRcRiUSoqKhgxowZzJw5s6tKEhFJG4a1o4lhe5E+EktSfZlN9WWuvf1xWLZTn52k+jKb6stsmsYgIiIiItJJCrsiIiIikrUUdkVEREQkaynsioiIiEjWUtgVERERkaylsCsiIiIiWUthV0RERESylsKuiIiIiGQthV0RERERyVr2zpx0//338/zzzwPJPdl//vOf79NGiYiIiIjsDbsc2V28eDGLFi3i6aefZu7cuSxfvpyXXnppf7RNRERERORb2eXIbklJCVOnTsXpdALQr18/Kisr93nDRERERES+rV2G3f79+6f+vnbtWubPn88TTzyxTxslIiIiIrI3dGrOLsCqVav48Y9/zA033ECfPn06/QBFRb5On1tS4u/0uZlI9WU21Ze5srk2ERHZuU6F3WXLlnHNNddw4403UlFRsVsPUFcXIJGwdnleSYmfmpqW3bp2JlF9mU31Za49qc00jd16oy4iIulrl2F38+bNXHXVVcyePZvhw4fvjzaJiIiIiOwVuwy7Dz/8MOFwmLvuuit17LzzzuP888/fpw0TEREREfm2dhl2p02bxrRp0/ZHW0REJMMkLItEwsJu0x5FIpKe1DuJiMge+/P8T/nDs8u7uhkiIh1Ku7D7z4Wr+Wx9Q1c3Q0REOiEYiVFZ29rVzRAR6VCnlx7bX159fyORWJwBBxR0dVNERGQXfB4HraFYVzdDRKRDaTey63baCYXjXd0MERHpBK/bQWswimXteolJEZGukIZh10YoolECEZFM4PXYiScsQhENUohIekrDsGsnqE5TRCQj+NwOAFpD0S5uiYjIjqVd2PW4NLIrIpIpvJ6tYTeofltE0lP6hV3N2RURyRhed/J7zgGN7IpImkq7sOt22ghqZFdEJCN8NbKrsCsi6Sn9wq5LI7siIpnCty3savkxEUlT6Rd2t47sahkbEZH0t20ag0Z2RSRdpV3Y9bjsWBZEYomuboqIiOyCw27D6TAJKOyKSJpKu7DrdtoACIX1kZiISCZI7qKmsCsi6Sntwq7HmfxITAuUi4hkhuQuahqgEJH0lHZhd9vIrlZkEBHJDF63XUuPiUjaSr+w69o6sqsVGUREMoLX49AX1EQkbaVf2NXIrohIRknO2VWfLSLpKe3CrselObsiIpkkOWc3qiUjRSQtpV/Y1WoMIiIZxeuxE09YGqQQkbSUdmHXrdUYREQyite9bRc1zdsVkfSTdmHX6TAxDM3ZFRHJFKktg7X8mIikIXtXN+CbDMPA7bRrNQYRkQzQumI53s2NAFp+TETSUtqFXQCPy6aRXRGRDND4yn8wq2rAO0bLj4lIWkq7aQyQnLerObsiIunPnpuLEWgG0PJjIpKW0jLsepw2rcYgIpIBbHn5JAItGFZCI7sikpbSMuy6nTaN7IqIZAB7Xh5YFgVGmIDCroikofQMuy47QYVdEZG0Z8/LB6DIFtXSYyKSltIz7DptBDWNQUQk7dm2hV0joqXHRCQtpWXY9egLaiKSpe6//34qKiqoqKhg5syZACxevJiJEydy8sknM3v27NS5K1eu5Mwzz2Ts2LHcdNNNxGLJMFlZWcmFF17IuHHjuPLKK2ltbQWgubmZyy+/nFNOOYULL7yQmpqafV6PPT8PgDwrpKXHRCQtpV3YXXfbdLqt+5BQJKZ91kUkqyxevJhFixbx9NNPM3fuXJYvX85zzz3HjTfeyAMPPMD8+fP55JNPeO211wCYMmUKN998My+88AKWZTFnzhwAbrvtNi644AIWLFjA4MGDeeCBBwC49957GTZsGM8//zxnn302M2bM2Oc12XOTYTc3EdQX1EQkLaVd2I01NuBrrMKyIBJNdHVzRET2mpKSEqZOnYrT6cThcNCvXz/Wrl1L79696dWrF3a7nYkTJ7JgwQI2bdpEKBRi6NChAEyaNIkFCxYQjUZ59913GTt2bLvjAAsXLmTixIkATJgwgddff51odN8GUMNux+b3440HtfSYiKSltNtUwub14YiGYOuWwS6nraubJCKyV/Tv3z/197Vr1zJ//nwuuugiSkpKUsdLS0upqqqiurq63fGSkhKqqqpoaGjA5/Nht9vbHQfa3cdut+Pz+aivr6esrKzTbSwq8nX63JISPwAbiwrxxUO0haIUF/swDKPT10hn2+rLVqovs6m+zku7sGt6vdgjIXChebsikpVWrVrFj3/8Y2644Qbsdjtr1qxpd7thGDucxrWz4x0xzd37AK+uLkAisespZCUlfmpqWpI/eP3YqxuIOSw2bGrE40q7/7Xstnb1ZSHVl9lU3/ZM0+jwzXraTWOweb3YI0EArcggIlln2bJl/PCHP+SnP/0pZ5xxBmVlZdTW1qZur66uprS0dLvjNTU1lJaWUlhYSCAQIB6PtzsOyVHhbfeJxWIEAgHy8/P3eU32vHwcbcn/MWn5MRFJN2kYdn0YoTZAI7sikl02b97MVVddxaxZs6ioqABgyJAhrFmzhnXr1hGPx3nuuec48cQT6dGjBy6Xi2XLlgEwd+5cTjzxRBwOB8OGDWP+/PntjgOMGjWKuXPnAjB//nyGDRuGw+HY53XZ8/MxgwGwLC0/JiJpJ+0+azJ9Pozg1rCrkV0RySIPP/ww4XCYu+66K3XsvPPO46677uLqq68mHA4zatQoxo0bB8CsWbOYNm0ara2tDBo0iMmTJwNwyy23MHXqVB588EG6devGPffcA8C1117L1KlTqaiowO/3M2vWrP1Sly0vDyORICeu5cdEJP2kXdi1eb0QCWNacY3sikhWmTZtGtOmTdvhbc8+++x2xwYOHMg///nP7Y736NGDxx57bLvj+fn5/P73v//2Dd1N23ZR88W1/JiIpJ9OT2MIBAJMmDCBjRs37sv2JMMu4I5HCEY0sisiku7sW+cFe2NafkxE0k+nwu6HH37I+eefz9q1a/dxc5JzdgE8ibBGdkVEMoBGdkUknXUq7M6ZM4dbbrkl9Y3ffcncOrKbk4hoNQYRkQxg+9qWwU2tkS5ujYhIe52as/tttpzc3QXKPT1L2QTk2WIYNjOrFk3Oplp2RPVltmyuL5trSwemw4mZk0N3V4w31zV0dXNERNrZ519Q290FyqOR5OLofiI0NAazZtFkLQCd2VRf5trbi5PLjtnz8imzx9hU20pNY5CSfE9XN0lEBEjDdXZNX/J/MF6imrMrIpIhbHl55FrJDYE++KJ2F2eLiOw/6Rd23W4wTbyWVmMQEckU9rx8jNYWuhXl8KHCroikkbQLu4ZhYPN68SQiBMMa2RURyQT2/DzijY0M6VfEZ+sb9QVjEUkbuxV2X3nlFXr27Lmv2pJier144mFCGtkVEckI9rwCrFiMoT29xBMWn6yp7+omiYgAaTiyC8m1dl0xrbMrIpIpti0/1isnjs/j4INVmsogIukhTcOuF0cspJFdEZEMsW1jCaulhcP6FvHxl3WdWolHRGRfS9Ow68MRCREKx7EsdZYiIuluW9iNNTYytH8xgWCUleu15q6IdL20DLumz4c9EsQCwlFNZRARSXf2rdMYYo2NHNa3kMJcF4/MX6kd1USky6Vl2LV5vZjRCKYV14oMIiIZwHR7MD0e2lZ8gstmcPWkwwm0Rfm/pz4mGkt0dfNE5DssbcMugDse0bxdEZEMUTzpbNpWrqD6749zQJmPSycM4otNTfz5+ZV8WdlMQ0tY83hFZL/b59sF7wlza9j1JLQig4hIpsgfPYZobQ0NLzyPo7iYo8eNZ9OIPjz75lqWLK8CIMdl59hBZYw8vBt9yv0YhtHFrRaRbJeWYdfmTW4Z7ImHCWlhchGRjFF85tlE6+qo/eccYo2NTJxwKkcPLKWmKURDS5hVGxpZ9PFmXn1/Ez2KvYw4rBvDB5eT53V2ddNFJEulddh1JyK0KeyKiGQMwzQpv/QyanI8NL78Es2LF1FYMZHDR/0XpruY0Uf0oC0U451Pq3jzo83MefUL/rlwNQeU+ehZ6qNXiY9uRTmUFuZQ4HPS3BqlMZD8lM9hN3E6THweB4V+N6apUWER2bU0DbvJaQw5iQjrqlo4akBpF7dIREQ6y3Q4KZv83+SP+T41/5xD7ZP/oH7eM+QeP5K8UaPxdO/Ofw3twX8N7UFlbStLlm/hy8pmPlhVy6KPNnfqMWymQVGumzyfE5/HQa7XSUm+h9J8D0V5bgAsC2LxBOFonHAkTjSWIBpPEI8nsNlMHHYT0zCobQpSVR+kuS1Cgd9FUa6bosIcqmoCBIJRvG4Hfbr56VOeS57PiWkYWJZFbVOINZubaWqN0LvMT59yP06HjVg8QVMggmVZeNx2PE77ToP5lvo2guEYvcv9mJrWIbLXpWXY3TZnt3sOfLa+sWsbIyIie8TVsxc9r/spwS+/pPGVl2h87VUaX/kP9sJCcgYdSs4hgyjpP4AzR/UDwLIsmtuiVNW3UVXfRmNrhNwcB/k+Fx6XnWg8QSQap7k1Qm1TiJrGIM2tEaobgqza2EQgGN3jtub5nOTmOPmysrnddbxuO23hGF9f8t1pNzFMg/A3vlNiMw28HgctrRG++TU8p8PE47ST47bTu9xPv+55OO0miz7ezKqNTak2HHlwCYV+F63BGG3hKE6HDZ/bgT/HQUmBh7KCHDwuO5tqAmyoDtDUGiGRsLAs8HrslOR7KMp1Y7eZWFjE4hZNgQhNrWFMw2DIQcUU+F2pdiUSFoZBu7nT4WicpkCYAr8bhz0tv8cuslvSM+x6PGCalOfAS5XNhKNxXA5bVzdLRET2gKdvXzx9f0zJ2ecSeP892lYsJ/DeMpoXvQGAo7gER0kppjcHW04Ohbm5lOblY/PnYjo8mIYbw3KAYYHDwiiyYXTzYroKMVxuTJcLwzQJhmNUNwRpaAmDASYWNixcHhcuhw2nw8RumthsBomERSSWHOUt8DpwtDaTiEZwdutOJJrAn+ch2BrCZpqEI3E2VAdYV9VCIBglHIkTiyfoXuylTzc/eV4Xazc380VlE4G2KAV+FwV+F6ZhEAzHaAvHCEXihCJxWtoirFzbwFtbv7BXVuDh7P/qR77PxXuranjz481EogmcdhOPy04ktvMlOE3DwDQNTAMinVni7YXPOKhnHt2Lfaze1MiWujZspkFhrps8r5O65hB1TSEswDCgOM9NzxIfQ/sXM/SgYgzD4OMv6/jkyzoisQRupw2Py/7ViHiumwK/KzUC3tIWpa45RDSW2DoNxUZRrgu3c8fxIxiOEQhGKc5z68uLstekZdg1DAOb10uxM0E8bPHlpiYO6VPY1c0SEZFvwZ6XT/5/jSH/v8ZgJRKEN24g+PlnBFd9TqyxkVhDPfHWVuKBFtjN3TMNlxt7QT6OomJKPTlEq6uIVG3BCofB4yHmzyXh2vYlOAMrkYBEAisaZWNDPcSTgdLZvTu5I04g7+ihtK3ZRKy+DsPppHtpGb17lxBvixOtriXWUI8tkoujpQirMU6PDz8g7/1lWNEYucePIG/gKJwlX03BS4TDhNatJbKpBkrttFkuQoad0oIcDKMFw2xl6GFuEkN7Y+bm4S4sSIW9WDxBS1uU6oY2qhqCBMMxehR76VnqI8/rTJ0XisSobUqG1eSIbTII53md5PmctIViLPusmqWf1fDpunq6FeZweL8iEgmLuuYwTYEwfbvnMvKwbhT4XdQ1h9hS38YXm5p4f1VtaopFwrLIzXHgz3ESjMRoC8W2WznJMMBmmsTiOw7gpfkeepR4U+EYAz75sp7PNzQST1h4XHZ6l/noUeyjtMBDSb6HYCRGTWOQ+uYwkBxJB2gNRWkNxYjHE7iddjwuGwf3KeLg7n66FSU/KW4MhKluCFJa4CHflxzZbm6NsHJdA1X1bVunt1j075WXCvWSPdIy7EJyKoPfiGIY8NmGRoVdEZEsYpgm7gN64z6gNwXfP7ndbVYiQbylmXhzC4lQkEQoRCIaTQYQw8CKx7EiYRLhrX9CIRLBNmINDUTr6ohWV+MoLSXv4IOxeX3EAwHiLc0kIlt3c7MsDNMGNhPDZsdXWIizpBQrEad5yWJqn/wHtU/+Y/fqsdvJGXQoGAYNC+bT8Py/seXnY9odYJpEa2sgsX3wq+zgeqbbjaOkFAyDRDgMiTjesnIO6dETe1ERieo2okvb2FxfT7RqM5GqKmw+P56D+tP7oINwlJZhz8/HlpeHYbdj2Azy3C4mHHcAE447gJKyPGprA52qzbIs1lW18P7ntQAMOaiYPt3azy8OhmPUNYWobQ7R2BKmoSVMNJ6gKNdNod+F02kjGk3On65qaGNjdYBNta2sWNeQmg7So9jLyUf3oiTfkxpJX7x883Yj27leJwYQ37pms9dtx+txYDMN6ppDtIViqaXuSvM9hLZOfdkmz+vE63FQWdv61etnJEfJF7yznkN6F3D+9/rT3BZhyfItrFzXQIHPRbdiLz1LfPTvmccBZT5spklDS5i1W5qxmSZlhR6K89zYTDP1vDW0hFlX1UJVfZDWUJS2UAybaVCS76E4300wHGNzXfJNjEFyuovLYcNpT34Sked1cmjfIkrzPTt8XbbNKf/6nPBYPEFjS5jWUIxAKEpujpOeJd7tArxlWdQ3h6luaMOf4yTf78LrtncY9Lc9ns/j2OtvBlpDUZx2E4d933yKn7Zh1+b1YQTbOKDUr3m7IiLfIYZpYs/Lx56Xv98fO/+/xhCurMQTaqbN7sFRUEgiEkmOFNdUY/N6cZaWYc8vIB5oIVpfjxWNkjNwIKY7GUiiDQ00L15EtLYGKxqFeBz/McfgPrAfrl4HQCJOIhhMhtitrK2jzFYkTLShgeiWLURrqsEwMF0uMAwimytp/HQlViy5SpHhdGLPy8dZXo5nwEBijY20ffYpLe+8tcs6V5kmpsuF6XaTHOmOJ99ERGNYseQbC/eBffEMGICzew/yQyFGtbWRCIex3gxTE4li8/lwFBfjKC7B5vNR5vbQvacXw1XULgxZsRiYJoa54/m/wXCMaCxB7g6Wn7Msi5ZglJqGIB6XneI8N85OTGs0HHZefnstn3xZj9dt54AyP2WFHqoagqzf0kJTW4TjBpUxqE8hB5T5sNtM4okEr763iblvrGH6n94BwOOycWifQgLBKB+trkt9gdLttOFy2mgKtN8O2zQMXE4Tm2mSSFjtVpQyDYMcd3Lu+dfne9tMIzVtIxL76suUX5+W0q0ohz7lueS47LicNhrbIqz4so7GQASH3aRHsZeiPDdV9UE217Wm3ghsU5TrYshBxXhcdlraIjS0JBcAaP7Gdt4el52+3XPp1z2X4rzk73PCsviyspkVa+upbQpR4HdxeL8iepf7qaxtZe2WltSnCfGERbeiHEYe3o2jB5bidtoJhmM0t0bwuO34PA4SCYv1VQE+39DI6som1m1pobYphMth47B+RQwbUMIpRb5dvsa7w7Cs3fysaDfV1QU6tWNOSYmfmpqW1M+bfjubWGMjS0b8gFfe28T/XX/CPkv8+8M368s2qi+zZXN9e1KbaRoU7eXONlPsaZ+dbdK1PisWI97aipmTg+lwbH+7ZRFrqCdWV0+sqYFYc3MqcFtfG1n2OE0C9U0kQiGwwLAlR7pNuwPD4SARiRD8YhXh9eu2G5E2nM7kOW1tO5xukgzheRgOB7HmZhKBAKbHg7vfQXgO6o89Px8ME8NuS20zbZgm0epqIls2k4iEcZZ1w9mtG64ePbH5/Tt+Lixr6yj99iH627x+LW0RXvugktICD0MPKm4Xrhtawny2oYHP1zcSjsbpU55Ln25+LAuqGtqoaQwSjiSIbX3Ouhd5OaDMR/diLzmu5KhpKsA3BvE47ZQWeLDbtq8hYVnUNAb56Is6Plpdy5b6IKFIjGA4TkmBhwPL/RxQ5qcxEGZDdYC65hBlBTn0LPFSVpiD3+PA63FQVd/GB1/UsnxNPbG4hT/HQZ7XSc9SH32759KtMIdAKEZDS5jNda2s3tTEpprWdl+y9LhsDDyggL7dc1m7uYXla+sJReI4HWbyjcTWGgzD4NN1DWypb8PpSIb+4DcCv81mpLYQL8l306c8l97lfmqbQrz3eQ3NrRGmXnw0B3fb8evekZ3122k9shvetJEBvfJ58d0NfFnZzIADCrq6WSIiIl3GsNux5+V1fLth4CgswlFYtNPrdDYMJkJBonX1mB4PthwPhtOVCpdWLEa0vp5obQ2JtlYSbUHiba3Em5qINTVhRaN4Dh6IPTeXWFMTwVWfUzf3qZ0/oM2GYbNhRb4acXQUl+Dq3RsrHidWX0+ssSE5whyJYNjteA87HP/Rx+IZOBCbJwdsNtrWb6B+4WLaVi7HzMnB1bMXrh49cZSW4SguTo6Wd8Cf42TC8X12eFuB38Vxg8o5blD5drcd3Ct/l88nJF+j3Jzk6h87YxoGZQU5nHR0Dicd3avdbbsT5g/ulc8JQ7oTTySS87g7MQUhGI7R+rVVSQpyXanpGZCcKlHXHGo3bWMby7L4YlMTb6+owsCgMM9FntdJMBynqTVMNJagb/c8Du6ZR56v/evwg5MOZnNdK4ceXEZjQyt7S9qGXdPrJdHaSv9e+Rgk5+0q7IqIiOw/ptuDq0ePHd5m2O04S0txlnZ+Lfx4WxuJUBDiCax4jEQwSLytDSsew1lahqO4BGw2Yg0NRDZXEt6wntDaNYTXr8dwOLAXFOLq3RubJwfD6STeGiDw3jIC7y376kFstq++cNijJ1ZNNYGl77Zrh83nx/S4MVxuDLs9OUJtWdjz8nD26ImrZ09sPj+Gw4FhsxFvaSbW2Egiklyxw9WjJ/bCwg7nriaiURJtrckpmXZ7csS9vp7Q2jVYkTD2ouT0D3t+fruRaSsWI9bUiL2waK/Pi/1mKN0Zj8uOx9VxRLTbTMoKcnZ4m2EY9O+ZT/+e+bvbREzToEeJb68veZe2Ydfm85EIhfA6koV/tr4RRnR1q0RERGRP2XKSy8vtiqOwEEdhId5DB+/y3NLzf0Bw1eeEN6xPflkxHKagdw+svgNxFCa/3J4IBQlXVhKtqSFaU02soZ5EKEwiHMKKxVLBMtrQQOuK5amwvFOGkQzDW6d+GA57MhgHAiRavxqVNH0+DIzkKiPfvITLjatXL5zl3YhWV20NwxFs/lxyBg3CWVZOpGoLkcpKDIcdz4BDyBl4CK7yQlrWbU5OEfHmYM8vwObPxYpFsSIRrHgc0+nCcLmw5eRg5uS0G5GPB9swTBuGwwGWlXyMzZuJbK5M/tmyBVtODr5hR+M74iii1VW0LH2XthXLMd1u7Pn52AsKU3O2k+G9GJvfT7S6mpZ33qL1ow9x9epFwcnjcJZ32+lTmYhGMAwz+cZjH0jfsLt1Y4l4aysDDsjnjQ8ricbiGT1vV0RERPYuwzTJGTCQnAEDU8e++TG/6fbg6dsPT99+u7yeFYsRqaoiEQqmgqPN70+OwtrsyRHnjRuJNTUmv1QYjZCIRpNfxIvFML2+5EoYOTnEW1uTUzriMdy9DsDV50BsOTnJVUNqqolUbiK8YQOBD97DWVJK3omjcJSWEVr9BW0rVtDy9lvYC4twdu+BFQ7R8OICGp7/N5t290kyTWw+H1Yslpxr3eGTaeAoLU2G79paav7+ODV/fzx5k9NJziGDIJEg1tBA8MvVJALtV/QwHI7kHHHA1edAmpcspumN18kZdCiG3U6ssZF4ayA5km4YWNEYibbW1H1Mjwdbbi7un10PBdtPFdlTaRt2za+F3SP7F/Pyso08++ba1E47IiIiInubYbd3OHUDwNP/YDz9D/5Wj7GrkU7GfB/LsrAikXbzixOhEMHVX5DndxGwHNh8vmSgbmwg3hLAdDgwtm6ykoiEscKR5Dzq5mbigRYMuwOb34+ZkwNbrw/gKC3D2a0bjtKydl98DFdW0vrRBzgKi/AOGbrdXOdEKEi0ppZoXfJPrLYWe0EBvmHH4CgsJNbcTOMr/6Hl7bcw3S5seQU4u3fHYOsUDZsNm9eLzevFiseJtwZIhMLYfd5v9fx+U9qGXZs3+Y26RGuAQ/p354TDuzF/yToO61vU6UngIiIiIpnIMAyMb4RL0+3Ge+hgCkr8xLaOXDuKiuGA3vukDa7u3XF1797h7abbg6tXL1y9eu3wdntuLsWnT6L49Em79bieEj+BvbgaStpuer0t7IYrk0tun//9/pTke3ho3graQrGd3VVEREREBEjjkV1Xz564eveh5u9/xVlaSs4hg/jRxEHc+df3mD3nA8oKc0hYFkW5bgYfWEi/Hnk7XKdORERERL670jbsGnY7Pa//GRt+fRebfncvPa+fQr/+/Tnvewex4J31NAYiGAa821LNv5esw+W0cewhZYw9pldqL+xtItE4DYEwsViCAr+bHPfOy47FE0RjiZ0uu9GRSDTO+qoAX1Y2kbBgUJ8CepUmR6mb2yJsqWvjgDIfbuf2145E46yrasEwDPp2z02thVfdGOS9z2roWerlkN4F7bYitKBTa+aJiIiIfBelbdiF5PJjPf93Chtm3snGe39DUcUExnzvJL4/7Ku5IcFwjJXrGvjgi1oWf7KFNz6sZHDfImymQX1ziPqWMIGvLYwMyW3+PK7kuneQXHvObjcxDWhpi6bO717s5eBe+fTrnkt5UQ7dCnNIbN0lpbo+yOb6VjbXJve0DkVihKNxWoMxEt/YUSbP68RmM6lvDqV+Pm3kgZwwpBtV9UHeWVnFx1/Ws76qJbXFX57PyRH9S6huaGPF2oZ21xrct5D65jAbawJEYgmOGVjKCYd3x5fj4L3Pa/h4dR3lRTlUDO+d2u5vVzbVtjLvzTXE4xbdir10L87hwPJcSgs8O1zrLxRJ7u+t1TFEREQknaXtdsFfF21ooPqxP9P60YfY8vMpmnAquSNGYjra7z7S1Brh5WUbeWv5FtxOG4W5bgr9Lgq2/tduM6lvCVHfHCYS/WoNvUTCIhpPEE9Y+HOc5PucGMAXm5pZtbGRUGTH6+2ZhkFJgYfyAg85bgdupw2vx0Gfcj99u+diWbB8TT0r1tbj8TgozXNTmOvmpaUb+GJjEzkuO23hGAbQr2ceB/fMp1+PXCLRBEs/q+bj1XX4c5ycMKQbxw0qY0N1gCXLq/hsfQMl+R56lfpIJCyWflZD+Gv19CzxsqW+DcuCEYeV43baqWkM0hgIYzNNHHaTHLedXiU+epX6+GRtPa+9X4nLaSPP66S6IZgK7D6Pg16lPmymQTxhEQzHqG0KEQhGMQ2DbkU59CrzYZgmG6taqG8O4fU4KMp1489x0BiIUN8cwjDgkN6FDD6wkIG9C/B5vvq2p2VZBIJRXA4bTocNy7Joao2woTpAYyCMaRiYpoE/x0F5YQ6FuW5Mw+j0CLxlWTS3RWloSW6luDsj9o2BMO+vqmXkET1xsOvfY8uyiMYShKNxEgmLXK9zry8M3lmRaJz3Pq8hEksw8rBumGbH7UjXrVH3Bm0XvHu0XXCS6stsqi+z7e1+OyPC7jZtn39G7b+eJLT6C2y5uRR8/2RyR5yw060Tv614IkF1Q5At9W1sqWvDZhqUFuRQUuChNN/T6V0+vl6fZVl8sKqWJSuq6N8zj2EDSinwb791YSyewDR3vbVfKBJj6ac1RGJxhh5UTGGum/rmEM8tWccbH1ZiMw1K8j0U+F0ktoaxptYI1Q1BIBnaRx3RndNHHog/x0k0lmBzXStrNjezelMzm2pbMYzkL5LbYaM4z01RnptwNMH6qhY21gTIcTso8rsozHPTGoymAnG+10lhrptwNM6n6xsIhpOhvDTfQ+9yP4FglA3VgdRouttpw2YatO7kS4jb5mbH4sm9tYvz3MkR+B55dC/KobQgh1Akxnuf1/DBF7VsqA4QiSbPddhNBh9YyOC+RTS2JPcTr28OYbMl3wT4PQ7Ki3IoL8zh03UNvLWiinjCwu20cdHJAxg+uJzapiDPLFrDl5XNHN6viGMHlWG3mbz58WaWLK+iufWrbS6HHlTMD08ZSK43+cZsc10rX1Y2U5TrprTAQ77ftdvTUGLxBC1tX31a0RaK0hiI0NQaJhSJE4kmqG5o452V1bRt3ZP88H5FXD7x0A6n8JSU+Hl96XreXlFFn3I/g/oUUNrB7jgd2VTbylvLt9AWinHikO70Lt/5vuatoSjVDUF6l/vbPQexeIJQJJ7aQ93l6NynB7F4gk01rayraqGytpWhBxUzsHeBwu5uUthNUn2ZTfVltu902IVkUAx+upL6BfNpW/4JAI6SUtx9++IoKsZ0uzE9HjBtGKYBhgEJC4vkVoAkLNj6dyt5QaxIJLnrSiSC6XBgut0YdgeJcIhEMJjcicTjxnR7MOxbRyQNtl4vQbunMB4nEU7uymLYbMldRYqKKSjJo7GuJbnoNCTbZRjJ3U6iUbDA9OZg8yYXfY7W1BCtrcH0eJJ7evfshc3vT+2AkohEiLc0kwgGMWw2sCd3bjFMG9hMDMME0ySWAIfLgWG3bzfCGAzH2FTTSq7XsdvBZk9ev1g8wZeVzaze1MSXlc2sq2rB63HQu8xH92If0Vic5tYo0VicbsVeepX4KMpzYwFWwqIxEGZLfXLaiAG4XXZMA9ZubuHzjY3tAuA2B3bzc1CPfIrz3eT7XKza0MjSz6pTc77LC3MoyfeQSFjE4gkaA5HUyLbLYWPk4d045pBSnl28juVf1jGgVz6rK5sAg4N65LJqY1Nq6onNNBhyUDEHdvPjcthoaYvy/NvryHE7OG3kgXy8uo4Pvqht1z6300bvMj+9y/3YTIPGQIRAMIrPY6cwN/mmotDvpjDXRVsoxpLlW3j3ayG2Iw67yVEHl3DCkO5sqW/jby99Tkm+hzFH9mBdVQvrtgQoL/Rw/GHdOKhHHv9+ez0vvLUOu81MvYnweRx4PQ68bjuWlQzVwUicg3vlU3Fcb3qX+2kNRXlreRWLPt7Mui0tmIaB3WYQiSXo3zOPw/sVJa/jduB02HDaTcLROG+vqGLpZzXE4gm6FSX3fS8vyOGtFVt499Pq1JsigIN65nHysF4ccXDxDre7TFgWby3fwpMLV9MUSL7RMIzkP89jDinlyrOGYkV3bwUXhV2FXdWX2VRfZvvOh92vC2/YQOvyjwl9uZrQmjXEmps6t8XfjhgGhtOFFQkn/y+57bDTmVycOdz++K6uZbrdWLFYaleQPW3Tdo9ps2HYbKmFoHfrcnb71i0Nk39MlxvT5Upuz5ccuk22eduOMV4v9rw8bP7crW1JgGFiOJ2YTmfyfqaJYRj4C3Npi5JazBrTSAZug+R9HA5sPj92vx/T6223F/jeYFkWdU0hqraOwttMg8P7FVGY697u3IRlUdMQJN/v2uGoYSyeoKYxSJ7XlRoJLSz08vDcj3n+7fUcP7iMU0ccSGGum0Awynuf1xCNJTj6kFJyc9pPrdlQHeCP85azqaYVn8fB947qybABJTS2Rqiub2NjbSvrtrSwviqAZVnk+5z4cpwE2qI0tIS3m//tdCRD7EE985PvtwCv206e10mez4XHmZwK4nLY2k1b+Gx9A//39Cdbg7SD3uV+1m1pIRCMJl8iA04++gBOO+FA6ptDrFjbwKaaAK2hGG2hKBgGXrcdh83kvVU1BMNx+pT72VjTSiye4IBSH8cf1o1jB5XhsBm88dFmXl62kdqm0A5frxyXneMOLaNnqY/X3q9kXVXy377LYeOoASUcUOYnkbAIRZIBv6YxRK7XiddtJ5GwsNtNSvM9lBfl8Pn6RlZXNnNgNz8nHd2LA7vlku9z8fxb65j/1nqcDpNfXX7cdq/NzijsKuyqvsym+jKbwu5OWKlR2iBWwkqGs61b0mGYGNv+r46RDGMYW0OuMxkADSN1DSsWS47w2pJhyEoksCJhrFh824OlAqJhbr0mJH/+2rXizc1E62rJ8zloCkST19s62oyVSO2rDRBvayPRFgDDxFFSiqOwkEQwSHjjBsIbN5AIBklEIhCPY/p82HNzMT0erHgiOWIcj2NZCax4HBKJ5KhzInnbV3+SI8mJSAQrHCYRDifva20d+bbZkkHWZifeGkhu7ZfazzsZeLe14dsw7HYMtzu524vNvl0QNxwOTIczGazdbky3G0wzWWMinhzFdmwN3Q5HKoCbXi82rw/T6cKKx7HisXbHU8HeMHZrLu22389Ewtrp3NcdicbirN7UzIHdczv8SD6RsLY2y2h3rDEQpr45TH1LMjQe3q9ohyt5dEZbKEZrKEpxnhtj65znj7+sY+XaBsaNOJDCHMeuL7L1Oq++v5F3V1ZzUM88Tjh8x1MWLMsiEkvQGozSGooRicaJxpKfhPTrkYdz63NhWRafb2ikuS3K4X2LcDnbP0eJhMWHq2t5d2U1sYSFaUA0lmBLfRvVDUF8HgdnjurH8YeVbzclpLoxyOrNLRw7oGS3XjeFXYVd1ZfZVF9mU9jNUNlWnxWLJUO1lQzUhX4XNZW1yRHwrUHeSiS2nmxtnXbRkpp6kZw2Ek7uK/71MB6NJt+wRKNb/x5OnhsKYSUSyZBrsyWD7B6MbqcYBqbXi933tW0TLQvT6cRRXIKjpATDZks9ttNMEGwKYMXi2HL92PMLsPn8GHZbMqw7nZg5OdhyvFvfIG27niv5psnlSk172fb4mGYy7Nu7flGUTP39jMUTqS8wdkRzdneP+uwk1ZfZVF9m29v9dtf/X1YykmG3twtpzkI/zvj+/XWyLCsZiLf+SYRDxFtbiQcCWNFIMoTabCQiERKtrcRbW7HiyVFsKxYjHggQD7SQCAaTI72mSTwYpHXFJ8QbG5MPYpqYbjd2jxvL7sSw2Yh/8Tnxlr24jaHHg83nT4birSPUViyWHH2PRr+aymKayZFslwsSia2fBLRh2GyYHg9mTk5yFNzlwnA4U28cMMCeX4C9oBDD6UiO6EfCYGwN2w4HUZ+bQEswOf/csti28IRhs2HYbVgJK/nGIxpNPh9bp7ekRsr5alTaSiQ/XbBiMRLBIPGW5J7spicHR0Ehttxc4m1txJuaSATbMNxubB4PhsuNYbcn27RtRN/pItbSTKy2llhzE/aCQpzl5dgLi5Ij4Vby8WLx+NZPNOJY8QQk4sl5+3YbMa+WxxMR+S5T2JWMZWydgoKz83MxOysRjUDCSk5xMYzt3mVasRjx1sDWqRJxrHA4FT6teAyMrRt/bBuZDoeTYdA02TYdBMsiEQ5/FbpDIaxIlEQkjOFwYM/NxXA4UtMuiCdIRMKpazmKi7F5crAS8eRoeTBIrK4u+XjRSGoaCIkEbStXJEP9V09eu/ngNXv9GfyGHc0/30/WOZ30+dVM7Pn5XfL4IiLStRR2RXbgm2s4f5Nht2PPy98/jdlLEqEgVjSW/BKhw5Ea4baiUYqL/dTWBb6ay7w1nG4L84aZ/GKiYbeTCIWINzcRa25OTsvYOgVkG8MwkiO+Nhs2jwebPxczJ4dEKESsoZ54czNmTg72vPzU8USwLTV/PPklyW3TV8LY/P5ksPfnEquvJ1K9hVjDVxutGKaZnDqydSTfsCVXImHrfPX8siISubld8ZSLiEgaUNgV+Y4w3R74+uIUXxsZt/u82IKJTl3HlpODLScHZ3m33Xr8bfejxzfa5XRCJ8OoPTcXd58+u/W4xVk+t01ERHauU+s/zZs3j/Hjx3PSSSfx+OOP7+s2iYjIt6A+W0TkK7sc2a2qqmL27Nk89dRTOJ1OzjvvPI499lgOOuig/dE+ERHZDeqzRUTa22XYXbx4Mccddxz5W7/cMXbsWBYsWMBPfvKTTj3A7q5tmc1UX2ZTfZlrd2vL5OdCffbeo/oym+rLbHuz395l2K2urqakpCT1c2lpKR999FGnH7ygwNvpc7N9XUvVl9lUX+bK5tq+SX323qP6Mpvqy2x7s75dztnd0Z4Tu7PzlIiI7D/qs0VE2ttl2C0rK6O2tjb1c3V1NaWlpfu0USIismfUZ4uItLfLsHv88cezZMkS6uvrCQaDvPjii5x44on7o20iIrKb1GeLiLS3yzm7ZWVlXH/99UyePJloNMpZZ53F4Ycfvj/aJiIiu0l9tohIe4a1owleIiIiIiJZoFObSoiIiIiIZCKFXRERERHJWgq7IiIiIpK1FHZFREREJGsp7IqIiIhI1urysDtv3jzGjx/PSSedxOOPP97Vzdkr7r//fioqKqioqGDmzJlAcr/6iRMncvLJJzN79uwubuHecffddzN16lQAVq5cyZlnnsnYsWO56aabiMViXdy6PffKK68wadIkxo0bxy9/+Usgu16/Z555JvX7effddwOZ//oFAgEmTJjAxo0bgY5fr0yvM11kW7+tPjuz/y2oz87M12+/9ttWF9qyZYs1evRoq6GhwWptbbUmTpxorVq1qiub9K29+eab1rnnnmuFw2ErEolYkydPtubNm2eNGjXKWr9+vRWNRq1LLrnEWrhwYVc39VtZvHixdeyxx1o33HCDZVmWVVFRYb3//vuWZVnWL37xC+vxxx/vwtbtufXr11sjR460Nm/ebEUiEev888+3Fi5cmDWvX1tbm3X00UdbdXV1VjQatc466yzrzTffzOjX74MPPrAmTJhgHXroodaGDRusYDDY4euVyXWmi2zrt9VnZ/a/BfXZmfn67e9+u0tHdhcvXsxxxx1Hfn4+OTk5jB07lgULFnRlk761kpISpk6ditPpxOFw0K9fP9auXUvv3r3p1asXdrudiRMnZnSdjY2NzJ49myuuuAKATZs2EQqFGDp0KACTJk3K2Ppeeuklxo8fT3l5OQ6Hg9mzZ+PxeLLm9YvH4yQSCYLBILFYjFgsht1uz+jXb86cOdxyyy2pLXE/+uijHb5e2fR72pWyrd9Wn53Z/xbUZ2fm67e/++1d7qC2L1VXV1NSUpL6ubS0lI8++qgLW/Tt9e/fP/X3tWvXMn/+fC666KLt6qyqquqK5u0V06dP5/rrr2fz5s3A9q9jSUlJxta3bt06HA4Hl156KTU1NYwePZr+/ftnzevn8/m49tprOeWUU3C73RxzzDE4HI6Mfv1mzJjR7ucd9StVVVVZ9XvalbKt31afndn/FtRnZ+brt7/77S4d2bV2sHmbYRhd0JK9b9WqVVxyySXccMMNHHDAAdvdnql1Pvnkk3Tr1o3hw4enjmXT6xiPx1myZAm//vWvmTNnDh9//HFqPtHXZWp9n376Kf/617949dVXWbRoEaZp8uabb253XqbWBx3/PmbT72lXytbnUX12ZtanPjspU+vbZl/32106sltWVsbSpUtTP1dXV6eGtDPZsmXLuOaaa7jxxhupqKjgnXfeoba2NnV7Jtc5f/58ampqOO2002hqaqKtrQ3DMNrVV1NTk7H1FRcXM3z4cAoLCwH43ve+x4IFC7DZbKlzMvn1W7RoEcOHD6eoqAhIfiT08MMPZ83rB8l+ZUf/3r55PNPr7CrZ2G+rz87cfwvqszP79dtmX/fbXTqye/zxx7NkyRLq6+sJBoO8+OKLnHjiiV3ZpG9t8+bNXHXVVcyaNYuKigoAhgwZwpo1a1i3bh3xeJznnnsuY+t85JFHeO6553jmmWe45pprGDNmDHfeeScul4tly5YBMHfu3Iytb/To0SxatIjm5mbi8ThvvPEG48aNy5rXb+DAgSxevJi2tjYsy+KVV17hmGOOyZrXDzr+99ajR4+sqrOrZFu/rT47s/8tqM/O7Ndvm33db3f5yO7111/P5MmTiUajnHXWWRx++OFd2aRv7eGHHyYcDnPXXXeljp133nncddddXH311YTDYUaNGsW4ceO6sJV736xZs5g2bRqtra0MGjSIyZMnd3WT9siQIUO47LLLuOCCC4hGo4wYMYLzzz+fvn37ZsXrN3LkSFasWMGkSZNwOBwcdthhXH755Zx00klZ8foBuFyuDv+9ZcvvaVfKtn5bfXZm/1tQn53Zr982+7rfNqwdTYgQEREREckCXb6phIiIiIjIvqKwKyIiIiJZS2FXRERERLKWwq6IiIiIZC2FXRERERHJWgq7IiIiIpK1FHZFREREJGv9f0RdAFAdH0HsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc60d791-b4e1-4f4d-abd2-9cf3b76c2d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"./modelo_mlp_regression.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c5788a1-c0e2-49e5-8871-aaff0ad3a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_p_train = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3956ec3-4073-4924-a1c9-ac3d6976e569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (root-mean-squared error): 11111.44333800608\n",
      "MAE (mean-absolute error): 7042.738980369947\n",
      "MSE (mean-squared error): 123464173.0537197\n",
      "Variance Score: 0.8407771629372156\n",
      "R2 score: 0.8329286214902809\n",
      "R2 score: 0.8407208803603584\n"
     ]
    }
   ],
   "source": [
    "print('RMSE (root-mean-squared error): {}'.format(math.sqrt(mean_squared_error(y_test,y_pred))))\n",
    "print('MAE (mean-absolute error): {}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "print('MSE (mean-squared error): {}'.format(mean_squared_error(y_test, y_pred)))\n",
    "print('Variance Score: {}'.format(explained_variance_score(y_test, y_pred)))\n",
    "print('R2 score: {}'.format(r2_score(y_train, y_p_train)))\n",
    "print('R2 score: {}'.format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad58e8-f4df-4ce0-811f-6c770c9ebb4e",
   "metadata": {},
   "source": [
    "Com a Rede MLP conseguimos um RMSE de 11111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128857a-13c9-4d57-adf2-6dfa1252447a",
   "metadata": {},
   "source": [
    "## Prevendo as próximas análises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5fce0-6893-4bb7-a836-b81b78784d8a",
   "metadata": {},
   "source": [
    "Vamos usar os Modelos, para prever as solicitações pendentes no momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72e58e4d-25ce-4e7e-8307-bdb4ead7e5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 268729.45594942,  313367.75314883,  452938.19351763, ...,\n",
       "        353472.28954204, 1040320.54890021,  307725.32908316])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_sem_Outliers.drop(['valorAprovado'], axis=1)\n",
    "Y = df_sem_Outliers.valorAprovado\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "lm = LinearRegression()\n",
    "linear = lm.fit(X_train, y_train.ravel())\n",
    "X_analise = df_analise.drop(['valorAprovado'], axis=1)\n",
    "Y_analise = lm.predict(X_analise)\n",
    "Y_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dedfe908-da7a-47c8-b109-11fab231626a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27222.518],\n",
       "       [28439.367],\n",
       "       [29514.896],\n",
       "       ...,\n",
       "       [28058.428],\n",
       "       [31639.201],\n",
       "       [28158.852]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ssc = ssc.fit_transform(X_analise)\n",
    "\n",
    "model = load_model(\"./modelo_mlp_regression.hdf5\")\n",
    "Y_model = model.predict(x_ssc)\n",
    "Y_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
